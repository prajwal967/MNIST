{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from joblib import Parallel, delayed\n",
    "import scipy\n",
    "import pandas as pd\n",
    "from numpy import array\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.misc\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000, 784)\n"
     ]
    }
   ],
   "source": [
    "#Loading the training and testing data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "print(X_train.shape)\n",
    "\n",
    "#Reshaping from(,28,28) to (,784) for training using MLP\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train -= np.mean(X_train)\n",
    "X_train /= np.std(X_train)\n",
    "X_test -= np.mean(X_train)\n",
    "X_test /= np.std(X_train)\n",
    "X_train = X_train.reshape(60000, 784)\n",
    "X_test = X_test.reshape(10000, 784)\n",
    "\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Selecting 6000 random examples from the test data\n",
    "test_rows = np.random.randint(0,X_test.shape[0],6000)\n",
    "X_test = X_test[test_rows]\n",
    "Y = y_test[test_rows]\n",
    "#Converting the output to binary classification(Six=1,Not Six=0)\n",
    "Y_test = Y == 6\n",
    "Y_test = Y_test.astype(int)\n",
    "\n",
    "#Selecting the examples where the output is 6\n",
    "X_six = X_train[y_train == 6]\n",
    "Y_six = y_train[y_train == 6]\n",
    "#Selecting the examples where the output is not 6\n",
    "X_not_six = X_train[y_train != 6]\n",
    "Y_not_six = y_train[y_train != 6]\n",
    "\n",
    "#Selecting 1000 random examples each from the data(Six and Not Six)\n",
    "random_rows = np.random.randint(0,X_six.shape[0],1000)\n",
    "X_six = X_six[random_rows]\n",
    "Y_six = Y_six[random_rows]\n",
    "random_rows = np.random.randint(0,X_not_six.shape[0],1000)\n",
    "X_not_six = X_not_six[random_rows]\n",
    "Y_not_six = Y_not_six[random_rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Appending the data with output as 6 and data with output as not six\n",
    "X_train = np.append(X_six,X_not_six)\n",
    "#Reshaping the appended data to appropraite form - (,784)\n",
    "X_train = X_train.reshape(X_six.shape[0] + X_not_six.shape[0], 784)\n",
    "#Appending the labels and converting the labels to binary classification(Six=1,Not Six=0)\n",
    "Y_labels = np.append(Y_six,Y_not_six)\n",
    "Y_train = Y_labels == 6 \n",
    "Y_train = Y_train.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Converting the classes to its binary categorical form\n",
    "nb_classes = 2\n",
    "Y_train = np_utils.to_categorical(Y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(Y_test, nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/prajwal967/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: DeprecationWarning: `imrotate` is deprecated!\n",
      "`imrotate` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.rotate`` instead.\n",
      "  \"\"\"\n",
      "/Users/prajwal967/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:9: DeprecationWarning: `imrotate` is deprecated!\n",
      "`imrotate` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.rotate`` instead.\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "#Reshaping to (28,28) and (1,28,28) is done because we are rotating images, so we cant use the (,784) dimension\n",
    "#array, we need to use the original image dimension array which is (28,28)\n",
    "\n",
    "#Initializing the array which will contain images rotated by 15 degrees anti clockwise\n",
    "anti_X_train = scipy.misc.imrotate(X_train[0].reshape(28,28), angle = 15)\n",
    "anti_X_train = anti_X_train.reshape(1, 28,28)\n",
    "\n",
    "#Initializing the array which will contain images rotated by 15 degrees clockwise\n",
    "clock_X_train = scipy.misc.imrotate(X_train[0].reshape(28,28), angle = -15)\n",
    "clock_X_train = clock_X_train.reshape(1, 28,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/prajwal967/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: DeprecationWarning: `imrotate` is deprecated!\n",
      "`imrotate` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.rotate`` instead.\n",
      "  \"\"\"\n",
      "/Users/prajwal967/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:8: DeprecationWarning: `imrotate` is deprecated!\n",
      "`imrotate` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.rotate`` instead.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#Performing clockwise and anticlockwise rotation for the rest of the images. Again reshaping needs to be done \n",
    "#below for the same reason as described above\n",
    "for i in range(1,2000):\n",
    "    \n",
    "    rotate_anti = scipy.misc.imrotate(X_train[i].reshape(28,28), angle = 15)\n",
    "    rotate_anti = rotate_anti.reshape(1, 28,28)\n",
    "    \n",
    "    rotate_clock = scipy.misc.imrotate(X_train[i].reshape(28,28), angle = -15)\n",
    "    rotate_clock = rotate_clock.reshape(1, 28,28)\n",
    "    \n",
    "    #Appending the rotated images to the resoective arrays\n",
    "    anti_X_train = np.append(anti_X_train,rotate_anti,axis=0)\n",
    "    clock_X_train = np.append(clock_X_train,rotate_clock,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Displaying the original and rotated images\n",
    "def image_compare(original,clockwise,anticlockwise):\n",
    "    \n",
    "    original = original.reshape(28,28)\n",
    "    \n",
    "    plt.figure(figsize=(20, 6))\n",
    "    ax = plt.subplot(1, 3, 1)\n",
    "    plt.imshow(original)\n",
    "    plt.xlabel('ORIGINAL')\n",
    "    plt.gray()\n",
    "    ax = plt.subplot(1, 3, 2)\n",
    "    plt.imshow(clockwise)\n",
    "    plt.xlabel('ROTATED CLOCKWISE')\n",
    "    plt.gray()\n",
    "    ax = plt.subplot(1, 3, 3)\n",
    "    plt.imshow(anticlockwise)\n",
    "    plt.xlabel('ROTATED ANTI-CLOCKWISE')\n",
    "    plt.gray()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABH8AAAF3CAYAAAA8QOdIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4xLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvAOZPmwAAIABJREFUeJzt3XuYpVV9J/rvD1raRrkZBIkBUQeJ\n1wBBjcgoxojo48QgkMg8UZJjYrzkaJzE6DEziTGJYzIRZ9RclOhojsbbMd5vQcBLkBjBEGlEJTrI\npQm0wQRQUC7r/FG7Q9l0V+/VXVW7atXn8zz9dNW7v3u9690b1m/3b7/73dVaCwAAAABj2m3WEwAA\nAABg6Wj+AAAAAAxM8wcAAABgYJo/AAAAAAPT/AEAAAAYmOYPAAAAwMA0fwAAAAAGpvkDAAAAMDDN\nHwAAAICBaf4AAAAADGzdcu6sqtpy7g9gNWit1aznsFKoEwB3pk7cQZ0AuLNp6oQzfwAAAAAGtkvN\nn6o6oaq+WlX/VFUvXaxJATAGdQKAhagTAMujWtu5MyeravckX0vyhCRXJvlCklNba19e4D5O0wTY\nyqin86sTAItDnfiB+6gTAFtZ6o99PSLJP7XWvtFa+36SdyZ56i6MB8BY1AkAFqJOACyTXWn+3DvJ\nFfN+v3Ky7QdU1bOr6vyqOn8X9gXA6qNOALAQdQJgmezKt31t67SiO52G2Vp7Y5I3Jk7TBFhj1AkA\nFqJOACyTXTnz58okB8/7/UeSbNq16QAwEHUCgIWoEwDLZFeaP19IclhV3beq9kjy9CQfXJxpATAA\ndQKAhagTAMtkpz/21Vq7tap+Ncknkuye5M2ttYsXbWYArGrqBAALUScAls9Of9X7Tu3MZ3QB7mTU\nr/DdGeoEwJ2pE3dQJwDubJo6sSsXfAYAAIAVabfd+q5ycuSRR06dPfjgg3ccmuf9739/Vx4W265c\n8wcAAACAFU7zBwAAAGBgmj8AAAAAA9P8AQAAABiY5g8AAADAwDR/AAAAAAam+QMAAAAwMM0fAAAA\ngIFp/gAAAAAMTPMHAAAAYGDVWlu+nVUt385gEa1fv74r/7a3vW3q7D777NM19vHHH9+VZ+VrrdWs\n57BSqBOw66r6lpTlfC3IzlEn7qBO0ONud7tbV/6jH/3o1Nnrr7++a+xTTz21K3/jjTd25VnbpqkT\nzvwBAAAAGJjmDwAAAMDANH8AAAAABqb5AwAAADAwzR8AAACAgWn+AAAAAAxM8wcAAABgYJo/AAAA\nAAPT/AEAAAAYmOYPAAAAwMA0fwAAAAAGtm7WE4DV4Dd/8ze78ieddNLU2Xe/+9290wFgDdmwYUNX\n/oEPfGBX/kUvelFXfuPGjVNn//AP/7BrbIDF1POaPEnufe97T50977zzusbeY489uvKw2Jz5AwAA\nADAwzR8AAACAgWn+AAAAAAxM8wcAAABgYJo/AAAAAAPT/AEAAAAYmOYPAAAAwMA0fwAAAAAGpvkD\nAAAAMDDNHwAAAICBaf4AAAAADGzdrCcAs3DIIYd05Z/znOd05S+++OKps8997nO7xgZg9duwYcPU\n2eOPP75r7Oc973ld+cMOO6wr31PjABbT3e52t678/e53v678bbfdNnX261//etfY//qv/9qVh8Xm\nzB8AAACAgWn+AAAAAAxM8wcAAABgYJo/AAAAAAPT/AEAAAAYmOYPAAAAwMA0fwAAAAAGpvkDAAAA\nMDDNHwAAAICBaf4AAAAADGzdrCcAs3D66ad35Q866KCu/BlnnDF19tvf/nbX2ACsfj/90z89dfa5\nz31u19gHHnhgV763Jn74wx/uygMslt717ZGPfGRXfvfdd586e9lll3WNffvtt3flYbE58wcAAABg\nYJo/AAAAAAPbpY99VdVlSW5IcluSW1trRy/GpAAYgzoBwELUCYDlsRjX/Hlca+1bizAOAGNSJwBY\niDoBsMR87AsAAABgYLva/GlJ/qaqLqiqZy/GhAAYijoBwELUCYBlsKsf+3p0a21TVR2Q5Myq+kpr\n7TPzA5NF3EIOsDapEwAsRJ0AWAa7dOZPa23T5O9rk7wvySO2kXlja+1oF28DWHvUCQAWok4ALI+d\nbv5U1d2qaq8tPyc5PsnGxZoYAKubOgHAQtQJgOWzKx/7OjDJ+6pqyzh/1Vr7+KLMCoARqBMALESd\nAFgmO938aa19I8mPLeJcABiIOgHAQtQJgOWzqxd8hhXj+OOPnzr75Cc/uWvsr3zlK135N77xjV15\nAFaW3XffvSt/7LHHduV/93d/d+rsQQcd1DX261//+q78O9/5zq78v/zLv3TlARYyOfNrKnvttVfX\n2A9/+MO78ldfffXU2d5/H/QcZ5K01rrysCO7+lXvAAAAAKxgmj8AAAAAA9P8AQAAABiY5g8AAADA\nwDR/AAAAAAam+QMAAAAwMM0fAAAAgIFp/gAAAAAMTPMHAAAAYGCaPwAAAAAD0/wBAAAAGNi6WU8A\nFssxxxwzdfaud71r19hf/vKXu/KbNm3qygOw9Kpq6uxhhx3WNfYzn/nMrvy97nWvqbMf/ehHu8Z+\nzWte05X/1re+1ZUHWEyttamzhx56aNfY1113XVe+Zz3cZ599usa+4ooruvKw2Jz5AwAAADAwzR8A\nAACAgWn+AAAAAAxM8wcAAABgYJo/AAAAAAPT/AEAAAAYmOYPAAAAwMA0fwAAAAAGpvkDAAAAMDDN\nHwAAAICBrZv1BGB7NmzY0JV/ylOeMnX2lltu6Rr79NNP78oDsPLsv//+U2dPOumkrrEf//jHd+U/\n//nPT519+9vf3jX2t771ra48wCxV1dTZPffcc8nGTpLLL7986uzmzZu7xoZZc+YPAAAAwMA0fwAA\nAAAGpvkDAAAAMDDNHwAAAICBaf4AAAAADEzzBwAAAGBgmj8AAAAAA9P8AQAAABiY5g8AAADAwDR/\nAAAAAAam+QMAAAAwsHWzngBsz8knn9yVP+qoo6bOvu997+sa+3Of+1xXHoCV52lPe9rU2ec85zld\nY1977bVd+de+9rVTZz/96U93jQ2wmqxfv35Jskmy9957d+U3b948dfaaa67pGhtmzZk/AAAAAAPT\n/AEAAAAYmOYPAAAAwMA0fwAAAAAGpvkDAAAAMDDNHwAAAICBaf4AAAAADEzzBwAAAGBgmj8AAAAA\nA9P8AQAAABiY5g8AAADAwNbNegKwPc94xjO68tddd93U2Ve+8pW90wFgiW3YsKEr/4hHPKIr/5zn\nPGfqbGuta+zf//3f78p/5CMf6coDjOr73//+1Nkf/dEf7Rp7jz326Mr3rM3r1vX9U/rWW2/tysNi\nc+YPAAAAwMB22PypqjdX1bVVtXHetntU1ZlVdenk7/2WdpoArFTqBAALUScAZm+aM3/ekuSErba9\nNMlZrbXDkpw1+R2AtektUScA2L63RJ0AmKkdNn9aa59JsvXFVJ6a5K2Tn9+a5GcWeV4ArBLqBAAL\nUScAZm9nr/lzYGvt6iSZ/H3A4k0JgAGoEwAsRJ0AWEZL/m1fVfXsJM9e6v0AsDqpEwAsRJ0A2HU7\ne+bPNVV1UJJM/r52e8HW2htba0e31o7eyX0BsPqoEwAsRJ0AWEY72/z5YJLTJj+fluQDizMdAAah\nTgCwEHUCYBlN81Xv70hyXpLDq+rKqnpWklcleUJVXZrkCZPfAViD1AkAFqJOAMzeDq/501o7dTs3\nPX6R5wLAKqROALAQdQJg9pb8gs+wxQknnNCVf8xjHtOV//SnPz119oILLugaG4Cld/jhh3fln/Ws\nZ3Xl991336mzb3rTm7rG/sQnPtGVB2DO+vXrp84efPDBXWNfeeWVXfnW2tTZW2+9tWtsmLWdveYP\nAAAAAKuA5g8AAADAwDR/AAAAAAam+QMAAAAwMM0fAAAAgIFp/gAAAAAMTPMHAAAAYGCaPwAAAAAD\n0/wBAAAAGJjmDwAAAMDANH8AAAAABrZu1hNg7Xj4wx/eld9jjz268h/5yEe68gCsLCeeeGJX/rjj\njuvKn3322VNn3/e+93WN/d3vfrcrD8Cc+9znPlNnd999966xb7rppq78unX+ecy4nPkDAAAAMDDN\nHwAAAICBaf4AAAAADEzzBwAAAGBgmj8AAAAAA9P8AQAAABiY5g8AAADAwDR/AAAAAAam+QMAAAAw\nMM0fAAAAgIFp/gAAAAAMbN2sJ8DacdJJJy3p+BdccMGSjg9AUlVd+R/+4R+eOvuMZzyja+z169d3\n5d/1rndNnb300ku7xgZg59x8881TZ/fcc8+usQ844ICu/A033NCV77H33nt35Q855JCps5s3b+4a\ne9999+3Kb9iwoSt/3XXXTZ3dtGlT19i33nprV547OPMHAAAAYGCaPwAAAAAD0/wBAAAAGJjmDwAA\nAMDANH8AAAAABqb5AwAAADAwzR8AAACAgWn+AAAAAAxM8wcAAABgYJo/AAAAAANbN+sJsLodcsgh\nS5JNknPPPXdJ8z0OPfTQrvwDHvCAqbMbN27sGnvTpk1deYDFdP/7378r/5KXvGTq7N5779019gc+\n8IGu/Kc+9amps9///ve7xt5///278rvtNv37b5s3b+4au7XWlQeYpcsvv3zqbO9r8ttuu60rf/XV\nV0+d3XfffbvGPuWUU7ryT3va06bOHn744V1j33TTTV35Sy+9tCt/9tlnT519z3ve0zV2z3PED3Lm\nDwAAAMDANH8AAAAABqb5AwAAADAwzR8AAACAgWn+AAAAAAxM8wcAAABgYJo/AAAAAAPT/AEAAAAY\nmOYPAAAAwMA0fwAAAAAGpvkDAAAAMLB1s54Aq9sv/dIvTZ3dd999u8b+xje+0ZU/4YQTps6efPLJ\nXWOfcsopXfm99tpr6uz111/fNfamTZu68q973eu68medddbU2a997WtdYwMrz5577tmVf/jDH96V\nP+KII6bOtta6xt6wYUNX/gUveMHU2cMPP7xr7N1263s/bePGjVNn3/nOd3aNfeWVV3blAWbpwAMP\nnDq7efPmrrFvuummrvzxxx8/dfa+971v19hPe9rTuvJ777331NmLLrqoa+wLL7ywK99Ty5PkSU96\n0tTZs88+u2vsq6++uivPHZz5AwAAADCwHTZ/qurNVXVtVW2ct+3lVXVVVV04+fPkpZ0mACuVOgHA\nQtQJgNmb5syftyTZ1udpXtNaO2Ly56OLOy0AVpG3RJ0AYPveEnUCYKZ22PxprX0myXXLMBcAViF1\nAoCFqBMAs7cr1/z51ar60uQ0zv0WbUYAjEKdAGAh6gTAMtnZ5s+fJbl/kiOSXJ3k1dsLVtWzq+r8\nqjp/J/cFwOqjTgCwEHUCYBntVPOntXZNa+221trtSc5I8ogFsm9srR3dWjt6ZycJwOqiTgCwEHUC\nYHntVPOnqg6a9+uJSTZuLwvA2qNOALAQdQJgea3bUaCq3pHkuCT7V9WVSX4nyXFVdUSSluSyJL+y\nhHMEYAVTJwBYiDoBMHs7bP601k7dxuY3LcFcAFiF1AkAFqJOAMzernzbFwAAAAArXLXWlm9nVcu3\nM5bF6173uqmzz3/+87vG/va3v92V32+/6b8h9JZbbuka+6qrrurK98z9rLPO6hr7nve8Z1f+tNNO\n68o/8YlPnDp75plndo3NtrXWatZzWCnUieX3oAc9qCv/rne9qyv/wAc+sCvf4zvf+U5Xfv369VNn\ne2vQbrv1vZ926623duV7vP71r+/Kv+c97+nKf/Ob35w621tv2TZ14g7qxHiOPfbYqbMvf/nLu8Z+\n2MMe1pW//vrrp84efPDBXWOfffbZXfmeevupT32qa+ybb765K/97v/d7XflHP/rRU2ePOuqorrF7\n575WTFMnnPkDAAAAMDDNHwAAAICBaf4AAAAADEzzBwAAAGBgmj8AAAAAA9P8AQAAABiY5g8AAADA\nwDR/AAAAAAam+QMAAAAwMM0fAAAAgIFp/gAAAAAMbN2sJ8DK8uM//uNd+V/+5V9eopkk++23X1f+\n3HPPnTr727/9211jn3POOV35pXTiiSd25U877bQlmgmwUu222/Tv7ey5555dYx922GFd+d13333q\n7Ic+9KGusb/5zW925T/+8Y9Pnb3hhhu6xn7oQx/ald+wYcPU2Qc/+MFdY59yyild+W984xtd+Wuu\nuWbq7C233NI1NrD2XHHFFVNnW2tdY++///5d+b322mvq7Kc+9amusV/84hd35Tdu3NiV7/GLv/iL\nXfmTTjqpK/+Rj3xk6mzv65Cbb765K88dnPkDAAAAMDDNHwAAAICBaf4AAAAADEzzBwAAAGBgmj8A\nAAAAA9P8AQAAABiY5g8AAADAwDR/AAAAAAam+QMAAAAwMM0fAAAAgIGtm/UEYHue+9znduXf8IY3\nLNFMltYv/MIvdOX/9E//tCt/zTXXdOXPPPPMrjyw8tx+++1TZ2+88causc8555yu/MMe9rCpsx/7\n2Me6xj7jjDO68rfddtvU2dZa19if+9znuvIHHHDA1Nmf+qmf6hq7t65cf/31Xfnvfve7XXmAhey1\n115TZ3/sx36sa+yq6sq/5z3vmTr74he/uGvs6667rit/4IEHTp097rjjusZ+2cte1pW/9tpru/If\n/vCHp85+5zvf6RqbnefMHwAAAICBaf4AAAAADEzzBwAAAGBgmj8AAAAAA9P8AQAAABiY5g8AAADA\nwDR/AAAAAAam+QMAAAAwMM0fAAAAgIFp/gAAAAAMTPMHAAAAYGDrZj0BVpYLLrigK3/GGWdMnX3+\n85/fNfbjH//4rvy73/3uqbMbNmzoGvvggw/uyp988slTZ3/913+9a+ybbrqpK/+iF72oKw+sLZs3\nb+7KX3XVVV35Rz7ykVNnH/WoR3WN/eUvf7kr3zP3e93rXl1jH3bYYV35JzzhCUuSTZJPfOITXflL\nLrmkK3/bbbd15QEW0vPa9uyzz+4au3f9vM997jN19phjjuka+7rrruvKP/GJT5w6+5SnPKVr7H32\n2acrf/rpp3flP/vZz06d/d73vtc1NjvPmT8AAAAAA9P8AQAAABiY5g8AAADAwDR/AAAAAAam+QMA\nAAAwMM0fAAAAgIFp/gAAAAAMTPMHAAAAYGCaPwAAAAAD0/wBAAAAGJjmDwAAAMDAqrW2fDurWr6d\nsSwe9KAHTZ0955xzusa+5z3v2ZW/4oorps7e9a53XdK59Ljwwgu78q94xSu68u9///u78iy/1lrN\neg4rhTqx8j32sY/tyr/yla+cOttTU5Kk9zXMLbfcMnV206ZNXWPvtddeXfn99ttv6uwnP/nJrrHP\nOOOMrnzv+Cw/deIO6sR4dttt+vMRjjrqqK6xn/e853Xln/rUp06d7a1Bt912W1d+jz32mDr7j//4\nj11jv+ENb+jKv+Md7+jKs/ymqRM7/D+tqg6uqnOq6pKquriqXjjZfo+qOrOqLp38Pf2rGACGoU4A\nsBB1AmD2pmmz3prk11trD0zyE0meX1UPSvLSJGe11g5LctbkdwDWHnUCgIWoEwAztsPmT2vt6tba\nFyc/35DkkiT3TvLUJG+dxN6a5GeWapIArFzqBAALUScAZq/rgs9VdWiSI5N8PsmBrbWrk7kFPckB\niz05AFYXdQKAhagTALOxbtpgVd09yXuT/Fpr7fqq6a47V1XPTvLsnZseAKuFOgHAQtQJgNmZ6syf\nqrpL5hbqt7fW/nqy+ZqqOmhy+0FJrt3WfVtrb2ytHd1aO3oxJgzAyqNOALAQdQJgtqb5tq9K8qYk\nl7TWTp930weTnDb5+bQkH1j86QGw0qkTACxEnQCYvWk+9vXoJM9IclFVXTjZ9rIkr0ry7qp6VpLL\nk5yyNFMEYIVTJwBYiDoBMGM7bP601v42yfY+kPv4xZ0OAKuNOgHAQtQJgNnr+rYvAAAAAFaXaq0t\n386qlm9nrDiPe9zjuvKveMUruvLHHnvs1Nne/+4/9KEPdeXf+973Tp39y7/8y66xGU9rbbqvO1kD\n1ImVb/369V35xz72sVNnn/70p3eN/ZM/+ZNd+csuu2zq7OWXX9419ubNm7vy559//tTZc889t2vs\na665piv/ve99ryvP8lMn7qBOrG13uctduvIPeMADuvLPfOYzp84+4QlP6Bp7v/3268p/8pOfnDr7\ntre9rWvsv//7v+/K33TTTV15lt80dcKZPwAAAAAD0/wBAAAAGJjmDwAAAMDANH8AAAAABqb5AwAA\nADAwzR8AAACAgWn+AAAAAAxM8wcAAABgYJo/AAAAAAPT/AEAAAAYmOYPAAAAwMCqtbZ8O6tavp0B\nrBKttZr1HFYKdYKldNBBB02dXb9+fdfYl112WedsYHrqxB3UCZbSAQccMHV23bp1XWNv2rSpdzow\ntWnqhDN/AAAAAAam+QMAAAAwMM0fAAAAgIFp/gAAAAAMTPMHAAAAYGCaPwAAAAAD0/wBAAAAGJjm\nDwAAAMDANH8AAAAABqb5AwAAADAwzR8AAACAgVVrbfl2VrV8OwNYJVprNes5rBTqBMCdqRN3UCcA\n7myaOuHMHwAAAICBaf4AAAAADEzzBwAAAGBgmj8AAAAAA9P8AQAAABiY5g8AAADAwDR/AAAAAAam\n+QMAAAAwMM0fAAAAgIFp/gAAAAAMTPMHAAAAYGCaPwAAAAAD0/wBAAAAGJjmDwAAAMDANH8AAAAA\nBqb5AwAAADAwzR8AAACAgWn+AAAAAAxM8wcAAABgYJo/AAAAAAPT/AEAAAAYmOYPAAAAwMA0fwAA\nAAAGpvkDAAAAMLAdNn+q6uCqOqeqLqmqi6vqhZPtL6+qq6rqwsmfJy/9dAFYadQJABaiTgDMXrXW\nFg5UHZTkoNbaF6tqryQXJPmZJD+b5MbW2h9PvbOqhXcGsAa11mrWc9gV6gTA0lInfmAsdQJgK9PU\niXVTDHJ1kqsnP99QVZckufeuTw+AEagTACxEnQCYva5r/lTVoUmOTPL5yaZfraovVdWbq2q/RZ4b\nAKuMOgHAQtQJgNmYuvlTVXdP8t4kv9Zauz7JnyW5f5IjMtfJf/V27vfsqjq/qs5fhPkCsEKpEwAs\nRJ0AmJ0dXvMnSarqLkk+nOQTrbXTt3H7oUk+3Fp7yA7G8RldgK2s9ms5JOoEwFJSJ34gp04AbGWa\nOjHNt31VkjcluWT+Qj25cNsWJybZuDOTBGB1UycAWIg6ATB703zb17FJPpvkoiS3Tza/LMmpmTtF\nsyW5LMmvTC7mttBYOvUAW1nt7+iqEwBLS534gbHUCYCtTFMnpvrY12KxWAPc2Wp/Ub+Y1AmAO1Mn\n7qBOANzZonzsCwAAAIDVS/MHAAAAYGCaPwAAAAAD0/wBAAAAGJjmDwAAAMDANH8AAAAABqb5AwAA\nADAwzR8AAACAgWn+AAAAAAxM8wcAAABgYJo/AAAAAAPT/AEAAAAYmOYPAAAAwMA0fwAAAAAGpvkD\nAAAAMDDNHwAAAICBaf4AAAAADEzzBwAAAGBgmj8AAAAAA9P8AQAAABiY5g8AAADAwDR/AAAAAAam\n+QMAAAAwMM0fAAAAgIGtW+b9fSvJN7exff/JbaNznONZK8fqOJfOfZZ5fyudOuE4R7JWjjNZO8eq\nTsyeOuE4R7JWjjNZO8e6YutEtdaWeiI7nkTV+a21o2c9j6XmOMezVo7VcTJra+W5cZxjWSvHmayd\nY10rx7karZXnxnGOZa0cZ7J2jnUlH6ePfQEAAAAMTPMHAAAAYGArpfnzxllPYJk4zvGslWN1nMza\nWnluHOdY1spxJmvnWNfKca5Ga+W5cZxjWSvHmaydY12xx7kirvkDAAAAwNJYKWf+AAAAALAEZtr8\nqaoTquqrVfVPVfXSWc5lqVXVZVV1UVVdWFXnz3o+i6Wq3lxV11bVxnnb7lFVZ1bVpZO/95vlHBfD\ndo7z5VV11eQ5vbCqnjzLOS6Gqjq4qs6pqkuq6uKqeuFk+1DP6QLHOdxzutqpE6ufOjHWmqJOjPec\nrnZrpU6MWiMSdWK0NUWdWLnP6cw+9lVVuyf5WpInJLkyyReSnNpa+/JMJrTEquqyJEe31r4167ks\npqp6TJIbk/xla+0hk21/lOS61tqrJkV4v9baS2Y5z121neN8eZIbW2t/PMu5LaaqOijJQa21L1bV\nXkkuSPIzSX4hAz2nCxznz2aw53Q1UyfGoE6MtaaoE+rESrKW6sSoNSJRJzLYmqJOrNw6Mcszfx6R\n5J9aa99orX0/yTuTPHWG82EntNY+k+S6rTY/NclbJz+/NXP/E6xq2znO4bTWrm6tfXHy8w1JLkly\n7wz2nC5wnKws6sQA1ImxqBOsMOrEANSJsagTK9csmz/3TnLFvN+vzAp/sHZRS/I3VXVBVT171pNZ\nYge21q5O5v6nSHLAjOezlH61qr40OY1zVZ+6uLWqOjTJkUk+n4Gf062OMxn4OV2F1IlxDbumbMOw\na4o6Md5zugqtpTqxlmpEMvCasg3DrinqxMp6TmfZ/KltbBv5q8ce3Vo7KsmTkjx/ctofq9ufJbl/\nkiOSXJ3k1bOdzuKpqrsneW+SX2utXT/r+SyVbRznsM/pKqVOsNoNu6aoE+M9p6vUWqoTasSYhl1T\n1ImV95zOsvlzZZKD5/3+I0k2zWguS661tmny97VJ3pe501RHdc3kM5BbPgt57YznsyRaa9e01m5r\nrd2e5IwM8pxW1V0yt4C9vbX215PNwz2n2zrOUZ/TVUydGNdwa8q2jLqmqBPjPaer2JqpE2usRiQD\nrinbMuqaok6szOd0ls2fLyQ5rKruW1V7JHl6kg/OcD5LpqruNrkIVKrqbkmOT7Jx4Xutah9Mctrk\n59OSfGCGc1kyWxaviRMzwHNaVZXkTUkuaa2dPu+moZ7T7R3niM/pKqdOjGuoNWV7RlxT1InxntNV\nbk3UiTVYI5LB1pTtGXFNUSdW7nM6s2/7SpKa+9qz/5lk9yRvbq39wcwms4Sq6n6Z69AnybokfzXK\nsVbVO5Icl2T/JNck+Z0k70/y7iSHJLk8ySmttVV9cbPtHOdxmTudryW5LMmvbPkc62pVVccm+WyS\ni5LcPtn8ssx9fnWY53SB4zw1gz2nq506sfqpE2OtKeqEOrHSrIU6MXKNSNSJDLamqBMrt07MtPkD\nAAAAwNKa5ce+AAAAAFhimj/grpHDAAAJvUlEQVQAAAAAA9P8AQAAABiY5g8AAADAwDR/AAAAAAam\n+cOKVlU/UlUfqKpLq+rrVfW/qmqPqjquqv6tqv6hqr5SVX887z6/UFWvn/f7z1fVl6rq4qr6x6r6\ni6rad3Lbp6rq6MnPl1XVe+fd7+SqestW8/lAVZ231baXV9VvLNFDALCiVdVtVXVhVW2sqg9tWV8n\ntz24qs6uqq9N1vH/VnN+cXKfC6vq+1V10eTnV8277w+st1X1W/Puc9u8n18wWYevmrftwqrad6ta\n8dWq+kxVPWWBY3lSVZ1fVZfMry3bW+e3V6Pm3f6IyT6/OhnvL6pqz/l1qqp2q6q3VtWbq+qFVfU/\n593/DVX1yXm//99V9drJzzfOu/9rJ4//RVX1haq67+S2y+Y9thduuS/Azhppzd/WfifbXl5V362q\nA+Ztu7Gqfmje/v55qznssWVd3s5+njl5zC6uqi9vqSlV9ZaqOnkb+W0+lvNu32G9qqq7VtWZVfU7\nVfWaqvq1eff/RFX9xbzfX11V/6WqDq2qjZNte1bV2yfP18aq+tuquvtW/x1s+fPShR5nVgbNH1as\nyQL310ne31o7LMkDktw9yR9MIp9trR2Z5MgkT6mqR29jjBOSvCjJk1prD05yVJLPJTlwO7s9uqoe\nvJ357Du5/75bXlgDkJtaa0e01h6S5Lokz0+SqtqQ5INJXtVae0CSH0tyTJLntdb+9+Q+RyTZlORx\nk99fOrnvndbb1tofzLvPln0e0Vrb0tB4zbxtR7TW/nWy/bOttSNba4cneUGS11fV47c+iKp6SJLX\nJ/n51toDkzwkyTe2d9A7qlFVdWCS9yR5yWTfD0zy8SR7bTXGnye5S5Jfylx9Ombebo5Isk9V7T75\n/Zgk5241lZ9L8sNJHtZae2iSE5P867zbHzfvMXnB9o4HYEpDrPnb2+8830ry6/M3tNb+Zd6c/nyr\nOXx/ew9YVT0pya8lOX7ev0f+bYH8dh/Lye07rFc190bEe5Nc0Fr73cyrL1W1W5L9k8z/N8+26ssL\nk1zTWnvo5Pl+VpJbJrfdtNXj/6qw4mn+sJL9ZJKbW2v/O0laa7dlrpHzfyXZc0uotXZTkguT3Hsb\nY/xWkt9orV21ZYzW2ptba1/dzj7/OMnLtnPbSUk+lOSdSZ7efzgAwzsvd6zF/znJua21v0mS1tp3\nk/xqkmneHVyS9ba1dmGSV0zmsbXfTPIHrbWvTLK3ttb+dIHhtlujqmrPzP2D6K2ttfMmt7fW2v/X\nWrtm3hj/K8kPJXlma+32JP+Q5AFVtaGq9kny3czVt4dO8sdk7gX8fAcluXpy/7TWrmytfXuaxwNg\nF63mNX9H+31zkp+rqnsswlT+n8z9e2TTZF43t9bOWCC/o8dyR/VqXeaO6dItDbbMNXa2vLnw4CQb\nk9xQVftV1frMvUHxD1vN46AkV235pbX21dba9zqOmxVG84eV7MFJLpi/obV2fZLLk/yHLduqar8k\nhyX5zHbG+GLHPt+d5Kiq+g/buO3UJO+Y/Dm1Y0yA4U3OTnl85t6tTLa9hn89yd2rau8dDLcz6+2L\n5p1+fs4CuS8m+dFtbH/I1vPdgR3VqB2N95+T/HiSp7fWbp3c/9bMNXsenuQnknw+yd8lOaaqfjhJ\ntdau2Gqcdyf5T5PjfnVVHbnV7efMe1xe1HF8ANs1wJq/o/3emLkG0AunnM9CFqO+zH8sdzTebya5\ntbX27x/zmjSebq2qQzLXBDovczXmUUmOTvKlbZy99OYkL6mq86rq96vqsHm3bdjqY18/13F8zIjm\nDytZJWkLbP+PVfWlJP+c5MOttX9ecLCqh04Wp68vsEDdluR/ZK5DP/++B2buxfzftta+lrnF8yF9\nhwMwpA1VdWGSf0lyjyRnTrZvbw3PAtt3Zb2df/r94xbI1QK39dhRjdqRLya5T5JHbLV9y7uzW16c\nnzf5+dG581k/aa1dmeTwzNWt25OctdVHHOZ/7Os1U8wLYCFDrPlT7ve1SU6bonm12HbqsZznb5M8\nqqoesNX27dWXbZ1VuuXMqftl7t9G90jyhap64OTmrT/29a4p5sWMaf6wkl2cuU70v5ssvgcn+Xrm\nPtP7sMydDv/cqjpiO2MclSSttYsmn9H9WJINC+z3/03ymCSHzNv2c0n2S/J/quqyJIfGR78AkskL\nwMw1MvbI5PoP2fYafr8kN7bWblhgvKVeb49Mcsk2tl+cuTNxprWjGrWj8b6S5GeTvGura81tuS7D\nozL3wvySJA/Ktq/HkCRprX2vtfax1tqLk7wyyc90HAdAj1HW/B3ud3Idob/K5Fo706qqP9hyRsxk\n02LUl/mP5Y7G+0zmrjH0sclZo1tsqS8PzdzHvv4uc7VmofpyY2vtr1trz0vytiRP7jgOVhjNH1ay\ns5LsWVXPTP799NJXJ3lL5q6DkCSZdOv/e5KXbGOM/57kj6vqR+ZtW6jxk9baLUlek7lFc4tTk5zQ\nWju0tXZoJqfqdx4PwLBaa/+WuYtr/kZV3SXJ25McW1U/lfz7BSxfm+SPdjDUkq23VfWwJP8tyZ9s\n4+b/keRlW94prblv0fovCwy33Ro1uT7D6zP3jvEj5+3/56vqXlt+b619Lslzknxkcip+Mvfi/CeS\n3LO1dm1rrSXZnOSp2cY7s1V11JYX95OLeD4syTd3/GgA7LwB1vxp93t6kl/J3HV0ptJa+615F4ZO\n5v498kdb1v+qWl9VC12Af0eP5Q7rVWvtvZPcx+uOb2Q7N8lTklw3uQ7qdUn2zR1vNvyAqnr05PIa\nWy4g/aCoL6ua5g8r1uQF74lJTqmqS5N8LcnN2fYFmf88yWO2vlJ/a+2jmVssP1ZzX6v4ucx9tOsT\nO9j9mzJZ5Kvq0MydBfR388b9P0mun/ei/r9W1ZVb/nQdKMAgWmv/kOQfM3cdm5sy17D4r1X11SQX\nJflC5poi2zTlers986//cOFkrGTuI8L/MJnDnyR5QWvtrG3M/UuZa/q/o6ouydy7ogfNi/zAOr+j\nGjW5sPPTM/cGxFcnY/7HJNdvtd8PJ/ndzL1A/6HJxZo3Z+6d3S3OS3JA5h7brR2Q5EM199W8X0py\na37wMZ5/zZ+/XOgBBOixWtf8nv221r6V5H1J1u9gPts1+ffInyT5ZFVdnLnr9cxvJr1hXn05b0eP\n5RT1ast+/zxz30r5waq662Sc/ecf92Tbv02Oc2v3T/LpqroocxeDPj9z3yCW3PmaP77taxWoudcu\nAAAAAIzImT8AAAAAA9P8AQAAABiY5g8AAADAwDR/AAAAAAam+QMAAAAwMM0fAAAAgIFp/gAAAAAM\nTPMHAAAAYGD/P0h7RcwNVFxLAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x181813cd30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_compare(X_train[0],clock_X_train[0],anti_X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABH8AAAF3CAYAAAA8QOdIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x181864cb70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_compare(X_train[1500],clock_X_train[1500],anti_X_train[1500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Reshaping the rotated arrays which have dimensions(,28,28) back to (,784) to train the arrays using MLP\n",
    "anti_X_train = anti_X_train.reshape(anti_X_train.shape[0], 784)\n",
    "clock_X_train = clock_X_train.reshape(clock_X_train.shape[0], 784)\n",
    "\n",
    "\n",
    "anti_X_train = anti_X_train.astype('float32')#(**)\n",
    "clock_X_train = clock_X_train.astype('float32')#(**)\n",
    "anti_X_train /= 255#(**)\n",
    "clock_X_train /= 255#(**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Appening the arrays to get the final rotated training array\n",
    "rotated_X_train = np.append(X_train, anti_X_train, axis = 0)\n",
    "rotated_Y_train = np.append(Y_train, Y_train, axis = 0)\n",
    "\n",
    "#The array that includes the original arrays as well as the 15 degree rotated arrays(2000 original, 2000 15 degree\n",
    "#clockwise rotated, 2000 15 degree anticlockwise rotated)\n",
    "rotated_X_train = np.append(rotated_X_train, clock_X_train, axis = 0)\n",
    "rotated_Y_train = np.append(rotated_Y_train, Y_train,axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function for constructing the multi layer perceptron\n",
    "# 1 Hidden Layer\n",
    "def build_layer_1(nb_epoch,X_train,Y_train):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, input_shape=(784,)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(nb_classes))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "    model.fit(X_train, Y_train, batch_size=128, nb_epoch=nb_epoch,verbose=1,\n",
    "              validation_data=(X_test, Y_test))\n",
    "          \n",
    "\n",
    "    score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "    print('Test score:', score[0])\n",
    "    print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2000 samples, validate on 6000 samples\n",
      "Epoch 1/1\n",
      "\r",
      " 128/2000 [>.............................] - ETA: 0s - loss: 0.7185 - acc: 0.6328"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/prajwal967/anaconda3/lib/python3.6/site-packages/keras/models.py:874: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 0s 148us/step - loss: 0.4799 - acc: 0.8835 - val_loss: 0.0731 - val_acc: 0.9737\n",
      "Test score: 0.0731047841869\n",
      "Test accuracy: 0.973666666667\n",
      "CPU times: user 1.8 s, sys: 129 ms, total: 1.93 s\n",
      "Wall time: 680 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "build_layer_1(1,X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/prajwal967/anaconda3/lib/python3.6/site-packages/keras/models.py:874: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6000 samples, validate on 6000 samples\n",
      "Epoch 1/1\n",
      "6000/6000 [==============================] - 1s 85us/step - loss: 0.2276 - acc: 0.9285 - val_loss: 0.0525 - val_acc: 0.9848\n",
      "Test score: 0.0524675939796\n",
      "Test accuracy: 0.984833333333\n",
      "CPU times: user 2.63 s, sys: 142 ms, total: 2.77 s\n",
      "Wall time: 953 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "build_layer_1(1,rotated_X_train,rotated_Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2000 samples, validate on 6000 samples\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/prajwal967/anaconda3/lib/python3.6/site-packages/keras/models.py:874: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 0s 237us/step - loss: 0.5968 - acc: 0.8825 - val_loss: 0.0477 - val_acc: 0.9850\n",
      "Epoch 2/5\n",
      "2000/2000 [==============================] - 0s 186us/step - loss: 0.0954 - acc: 0.9650 - val_loss: 0.0479 - val_acc: 0.9850\n",
      "Epoch 3/5\n",
      "2000/2000 [==============================] - 0s 236us/step - loss: 0.0580 - acc: 0.9795 - val_loss: 0.0550 - val_acc: 0.9812\n",
      "Epoch 4/5\n",
      "2000/2000 [==============================] - 0s 175us/step - loss: 0.1024 - acc: 0.9680 - val_loss: 0.0431 - val_acc: 0.9827\n",
      "Epoch 5/5\n",
      "2000/2000 [==============================] - 0s 131us/step - loss: 0.0172 - acc: 0.9935 - val_loss: 0.0886 - val_acc: 0.9708\n",
      "Test score: 0.0886092962222\n",
      "Test accuracy: 0.970833333333\n",
      "CPU times: user 7.3 s, sys: 508 ms, total: 7.81 s\n",
      "Wall time: 2.51 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "build_layer_1(5,X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6000 samples, validate on 6000 samples"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/prajwal967/anaconda3/lib/python3.6/site-packages/keras/models.py:874: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/5\n",
      "6000/6000 [==============================] - 1s 103us/step - loss: 0.2028 - acc: 0.9322 - val_loss: 0.0997 - val_acc: 0.9705\n",
      "Epoch 2/5\n",
      "6000/6000 [==============================] - 0s 83us/step - loss: 0.0879 - acc: 0.9727 - val_loss: 0.0364 - val_acc: 0.9878\n",
      "Epoch 3/5\n",
      "6000/6000 [==============================] - 1s 128us/step - loss: 0.0620 - acc: 0.9790 - val_loss: 0.0590 - val_acc: 0.9832\n",
      "Epoch 4/5\n",
      "6000/6000 [==============================] - 1s 108us/step - loss: 0.0467 - acc: 0.9845 - val_loss: 0.0311 - val_acc: 0.9903\n",
      "Epoch 5/5\n",
      "6000/6000 [==============================] - 1s 124us/step - loss: 0.0325 - acc: 0.9903 - val_loss: 0.0654 - val_acc: 0.9847\n",
      "Test score: 0.0653694951598\n",
      "Test accuracy: 0.984666666667\n"
     ]
    }
   ],
   "source": [
    "build_layer_1(5,rotated_X_train,rotated_Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/prajwal967/anaconda3/lib/python3.6/site-packages/keras/models.py:874: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2000 samples, validate on 6000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 0s 186us/step - loss: 0.4013 - acc: 0.8785 - val_loss: 0.0817 - val_acc: 0.9713\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 112us/step - loss: 0.0741 - acc: 0.9745 - val_loss: 0.0744 - val_acc: 0.9723\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 114us/step - loss: 0.0817 - acc: 0.9735 - val_loss: 0.0754 - val_acc: 0.9747\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 113us/step - loss: 0.0326 - acc: 0.9895 - val_loss: 0.0387 - val_acc: 0.9880\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 144us/step - loss: 0.0516 - acc: 0.9790 - val_loss: 0.0965 - val_acc: 0.9690\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 191us/step - loss: 0.0229 - acc: 0.9920 - val_loss: 0.0498 - val_acc: 0.9818\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 151us/step - loss: 0.0084 - acc: 0.9985 - val_loss: 0.1393 - val_acc: 0.9573\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 120us/step - loss: 0.0321 - acc: 0.9880 - val_loss: 0.0562 - val_acc: 0.9825\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 122us/step - loss: 0.0106 - acc: 0.9950 - val_loss: 0.0555 - val_acc: 0.9833\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 123us/step - loss: 0.0019 - acc: 1.0000 - val_loss: 0.0666 - val_acc: 0.9802\n",
      "Test score: 0.0666088786335\n",
      "Test accuracy: 0.980166666667\n",
      "CPU times: user 10.5 s, sys: 593 ms, total: 11.1 s\n",
      "Wall time: 3.41 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "build_layer_1(10,X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/prajwal967/anaconda3/lib/python3.6/site-packages/keras/models.py:874: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6000 samples, validate on 6000 samples\n",
      "Epoch 1/10\n",
      "6000/6000 [==============================] - 1s 198us/step - loss: 0.2195 - acc: 0.9248 - val_loss: 0.0459 - val_acc: 0.9863\n",
      "Epoch 2/10\n",
      "6000/6000 [==============================] - 1s 90us/step - loss: 0.0840 - acc: 0.9732 - val_loss: 0.0886 - val_acc: 0.9747\n",
      "Epoch 3/10\n",
      "6000/6000 [==============================] - 1s 92us/step - loss: 0.0609 - acc: 0.9788 - val_loss: 0.0758 - val_acc: 0.9785\n",
      "Epoch 4/10\n",
      "6000/6000 [==============================] - 1s 85us/step - loss: 0.0450 - acc: 0.9847 - val_loss: 0.1334 - val_acc: 0.9692\n",
      "Epoch 5/10\n",
      "6000/6000 [==============================] - 1s 86us/step - loss: 0.0325 - acc: 0.9892 - val_loss: 0.0395 - val_acc: 0.9908\n",
      "Epoch 6/10\n",
      "6000/6000 [==============================] - 1s 87us/step - loss: 0.0250 - acc: 0.9920 - val_loss: 0.0667 - val_acc: 0.9867\n",
      "Epoch 7/10\n",
      "6000/6000 [==============================] - 0s 83us/step - loss: 0.0183 - acc: 0.9942 - val_loss: 0.0726 - val_acc: 0.9862\n",
      "Epoch 8/10\n",
      "6000/6000 [==============================] - 0s 78us/step - loss: 0.0136 - acc: 0.9957 - val_loss: 0.0666 - val_acc: 0.9868\n",
      "Epoch 9/10\n",
      "6000/6000 [==============================] - 0s 77us/step - loss: 0.0131 - acc: 0.9953 - val_loss: 0.0837 - val_acc: 0.9858\n",
      "Epoch 10/10\n",
      "6000/6000 [==============================] - 0s 77us/step - loss: 0.0094 - acc: 0.9973 - val_loss: 0.1113 - val_acc: 0.9845\n",
      "Test score: 0.111268735559\n",
      "Test accuracy: 0.9845\n",
      "CPU times: user 19.5 s, sys: 1.23 s, total: 20.7 s\n",
      "Wall time: 6.29 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "build_layer_1(10,rotated_X_train,rotated_Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2000 samples, validate on 6000 samples"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/prajwal967/anaconda3/lib/python3.6/site-packages/keras/models.py:874: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/30\n",
      "2000/2000 [==============================] - 0s 218us/step - loss: 0.4257 - acc: 0.8820 - val_loss: 0.1416 - val_acc: 0.9533\n",
      "Epoch 2/30\n",
      "2000/2000 [==============================] - 0s 119us/step - loss: 0.0773 - acc: 0.9755 - val_loss: 0.0839 - val_acc: 0.9697\n",
      "Epoch 3/30\n",
      "2000/2000 [==============================] - 0s 119us/step - loss: 0.0624 - acc: 0.9795 - val_loss: 0.1060 - val_acc: 0.9662\n",
      "Epoch 4/30\n",
      "2000/2000 [==============================] - 0s 119us/step - loss: 0.0501 - acc: 0.9795 - val_loss: 0.0318 - val_acc: 0.9887\n",
      "Epoch 5/30\n",
      "2000/2000 [==============================] - 0s 119us/step - loss: 0.0405 - acc: 0.9835 - val_loss: 0.0888 - val_acc: 0.9697\n",
      "Epoch 6/30\n",
      "2000/2000 [==============================] - 0s 117us/step - loss: 0.0160 - acc: 0.9950 - val_loss: 0.0363 - val_acc: 0.9893\n",
      "Epoch 7/30\n",
      "2000/2000 [==============================] - 0s 117us/step - loss: 0.0349 - acc: 0.9870 - val_loss: 0.0373 - val_acc: 0.9865\n",
      "Epoch 8/30\n",
      "2000/2000 [==============================] - 0s 124us/step - loss: 0.0047 - acc: 0.9990 - val_loss: 0.0535 - val_acc: 0.9835\n",
      "Epoch 9/30\n",
      "2000/2000 [==============================] - 0s 122us/step - loss: 0.0394 - acc: 0.9850 - val_loss: 0.0565 - val_acc: 0.9838\n",
      "Epoch 10/30\n",
      "2000/2000 [==============================] - 0s 122us/step - loss: 0.0023 - acc: 1.0000 - val_loss: 0.0523 - val_acc: 0.9847\n",
      "Epoch 11/30\n",
      "2000/2000 [==============================] - 0s 122us/step - loss: 0.0017 - acc: 1.0000 - val_loss: 0.0422 - val_acc: 0.9863\n",
      "Epoch 12/30\n",
      "2000/2000 [==============================] - 0s 120us/step - loss: 0.0412 - acc: 0.9895 - val_loss: 0.0497 - val_acc: 0.9855\n",
      "Epoch 13/30\n",
      "2000/2000 [==============================] - 0s 119us/step - loss: 9.8379e-04 - acc: 1.0000 - val_loss: 0.0528 - val_acc: 0.9855\n",
      "Epoch 14/30\n",
      "2000/2000 [==============================] - 0s 114us/step - loss: 8.6039e-04 - acc: 1.0000 - val_loss: 0.0783 - val_acc: 0.9803\n",
      "Epoch 15/30\n",
      "2000/2000 [==============================] - 0s 120us/step - loss: 0.0015 - acc: 1.0000 - val_loss: 0.0488 - val_acc: 0.9858\n",
      "Epoch 16/30\n",
      "2000/2000 [==============================] - 0s 118us/step - loss: 0.0134 - acc: 0.9970 - val_loss: 0.0624 - val_acc: 0.9847\n",
      "Epoch 17/30\n",
      "2000/2000 [==============================] - 0s 125us/step - loss: 0.0011 - acc: 0.9995 - val_loss: 0.0648 - val_acc: 0.9828\n",
      "Epoch 18/30\n",
      "2000/2000 [==============================] - 0s 120us/step - loss: 3.8213e-04 - acc: 1.0000 - val_loss: 0.0645 - val_acc: 0.9845\n",
      "Epoch 19/30\n",
      "2000/2000 [==============================] - 0s 119us/step - loss: 2.4976e-04 - acc: 1.0000 - val_loss: 0.0649 - val_acc: 0.9840\n",
      "Epoch 20/30\n",
      "2000/2000 [==============================] - 0s 118us/step - loss: 2.7031e-04 - acc: 1.0000 - val_loss: 0.0570 - val_acc: 0.9862\n",
      "Epoch 21/30\n",
      "2000/2000 [==============================] - 0s 117us/step - loss: 0.0084 - acc: 0.9965 - val_loss: 0.0432 - val_acc: 0.9893\n",
      "Epoch 22/30\n",
      "2000/2000 [==============================] - 0s 119us/step - loss: 6.6913e-04 - acc: 1.0000 - val_loss: 0.0807 - val_acc: 0.9830\n",
      "Epoch 23/30\n",
      "2000/2000 [==============================] - 0s 120us/step - loss: 7.7246e-05 - acc: 1.0000 - val_loss: 0.0736 - val_acc: 0.9847\n",
      "Epoch 24/30\n",
      "2000/2000 [==============================] - 0s 117us/step - loss: 5.6589e-04 - acc: 1.0000 - val_loss: 0.0787 - val_acc: 0.9845\n",
      "Epoch 25/30\n",
      "2000/2000 [==============================] - 0s 117us/step - loss: 9.3392e-05 - acc: 1.0000 - val_loss: 0.0781 - val_acc: 0.9842\n",
      "Epoch 26/30\n",
      "2000/2000 [==============================] - 0s 123us/step - loss: 2.7616e-05 - acc: 1.0000 - val_loss: 0.0823 - val_acc: 0.9843\n",
      "Epoch 27/30\n",
      "2000/2000 [==============================] - 0s 119us/step - loss: 0.0145 - acc: 0.9950 - val_loss: 0.0999 - val_acc: 0.9838\n",
      "Epoch 28/30\n",
      "2000/2000 [==============================] - 0s 118us/step - loss: 5.6332e-05 - acc: 1.0000 - val_loss: 0.0924 - val_acc: 0.9838\n",
      "Epoch 29/30\n",
      "2000/2000 [==============================] - 0s 117us/step - loss: 5.2184e-05 - acc: 1.0000 - val_loss: 0.0932 - val_acc: 0.9838\n",
      "Epoch 30/30\n",
      "2000/2000 [==============================] - 0s 123us/step - loss: 2.0786e-05 - acc: 1.0000 - val_loss: 0.0850 - val_acc: 0.9847\n",
      "Test score: 0.0850140756318\n",
      "Test accuracy: 0.984666666667\n",
      "CPU times: user 27.2 s, sys: 1.38 s, total: 28.5 s\n",
      "Wall time: 7.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "build_layer_1(30,X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6000 samples, validate on 6000 samples"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/prajwal967/anaconda3/lib/python3.6/site-packages/keras/models.py:874: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/30\n",
      "6000/6000 [==============================] - 1s 112us/step - loss: 0.2476 - acc: 0.9262 - val_loss: 0.0420 - val_acc: 0.9860\n",
      "Epoch 2/30\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.0981 - acc: 0.9680 - val_loss: 0.0410 - val_acc: 0.9867\n",
      "Epoch 3/30\n",
      "6000/6000 [==============================] - 0s 73us/step - loss: 0.0629 - acc: 0.9793 - val_loss: 0.0309 - val_acc: 0.9903\n",
      "Epoch 4/30\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.0420 - acc: 0.9863 - val_loss: 0.0301 - val_acc: 0.9910\n",
      "Epoch 5/30\n",
      "6000/6000 [==============================] - 1s 89us/step - loss: 0.0359 - acc: 0.9867 - val_loss: 0.0823 - val_acc: 0.9800\n",
      "Epoch 6/30\n",
      "6000/6000 [==============================] - 1s 87us/step - loss: 0.0285 - acc: 0.9893 - val_loss: 0.0431 - val_acc: 0.9912\n",
      "Epoch 7/30\n",
      "6000/6000 [==============================] - 1s 90us/step - loss: 0.0200 - acc: 0.9930 - val_loss: 0.0470 - val_acc: 0.9902\n",
      "Epoch 8/30\n",
      "6000/6000 [==============================] - 0s 77us/step - loss: 0.0156 - acc: 0.9957 - val_loss: 0.0838 - val_acc: 0.9860\n",
      "Epoch 9/30\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 0.0136 - acc: 0.9955 - val_loss: 0.0604 - val_acc: 0.9887\n",
      "Epoch 10/30\n",
      "6000/6000 [==============================] - 0s 79us/step - loss: 0.0070 - acc: 0.9982 - val_loss: 0.0702 - val_acc: 0.9887\n",
      "Epoch 11/30\n",
      "6000/6000 [==============================] - 0s 81us/step - loss: 0.0082 - acc: 0.9975 - val_loss: 0.0668 - val_acc: 0.9885\n",
      "Epoch 12/30\n",
      "6000/6000 [==============================] - 0s 77us/step - loss: 0.0055 - acc: 0.9982 - val_loss: 0.0378 - val_acc: 0.9910\n",
      "Epoch 13/30\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.0056 - acc: 0.9983 - val_loss: 0.0829 - val_acc: 0.9872\n",
      "Epoch 14/30\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.0029 - acc: 0.9998 - val_loss: 0.1117 - val_acc: 0.9847\n",
      "Epoch 15/30\n",
      "6000/6000 [==============================] - 0s 72us/step - loss: 0.0039 - acc: 0.9988 - val_loss: 0.0752 - val_acc: 0.9898\n",
      "Epoch 16/30\n",
      "6000/6000 [==============================] - 0s 77us/step - loss: 0.0017 - acc: 0.9998 - val_loss: 0.0779 - val_acc: 0.9885\n",
      "Epoch 17/30\n",
      "6000/6000 [==============================] - 0s 82us/step - loss: 0.0025 - acc: 0.9997 - val_loss: 0.0795 - val_acc: 0.9893\n",
      "Epoch 18/30\n",
      "6000/6000 [==============================] - 0s 79us/step - loss: 0.0031 - acc: 0.9993 - val_loss: 0.0600 - val_acc: 0.9918\n",
      "Epoch 19/30\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 7.9926e-04 - acc: 1.0000 - val_loss: 0.0924 - val_acc: 0.9892\n",
      "Epoch 20/30\n",
      "6000/6000 [==============================] - 0s 80us/step - loss: 0.0027 - acc: 0.9993 - val_loss: 0.1372 - val_acc: 0.9833\n",
      "Epoch 21/30\n",
      "6000/6000 [==============================] - 0s 77us/step - loss: 0.0047 - acc: 0.9992 - val_loss: 0.0875 - val_acc: 0.9905\n",
      "Epoch 22/30\n",
      "6000/6000 [==============================] - 0s 76us/step - loss: 0.0013 - acc: 0.9998 - val_loss: 0.1157 - val_acc: 0.9865\n",
      "Epoch 23/30\n",
      "6000/6000 [==============================] - 0s 79us/step - loss: 0.0011 - acc: 0.9997 - val_loss: 0.0900 - val_acc: 0.9895\n",
      "Epoch 24/30\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 0.0016 - acc: 0.9997 - val_loss: 0.1619 - val_acc: 0.9800\n",
      "Epoch 25/30\n",
      "6000/6000 [==============================] - 0s 77us/step - loss: 2.0648e-04 - acc: 1.0000 - val_loss: 0.0949 - val_acc: 0.9905\n",
      "Epoch 26/30\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 3.2379e-04 - acc: 1.0000 - val_loss: 0.1220 - val_acc: 0.9863\n",
      "Epoch 27/30\n",
      "6000/6000 [==============================] - 0s 75us/step - loss: 2.0823e-04 - acc: 1.0000 - val_loss: 0.1007 - val_acc: 0.9883\n",
      "Epoch 28/30\n",
      "6000/6000 [==============================] - 0s 78us/step - loss: 8.4282e-04 - acc: 0.9998 - val_loss: 0.0901 - val_acc: 0.9903\n",
      "Epoch 29/30\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 9.8774e-04 - acc: 0.9997 - val_loss: 0.1636 - val_acc: 0.9837\n",
      "Epoch 30/30\n",
      "6000/6000 [==============================] - 0s 74us/step - loss: 3.4519e-04 - acc: 0.9998 - val_loss: 0.0867 - val_acc: 0.9903\n",
      "Test score: 0.0867010157954\n",
      "Test accuracy: 0.990333333333\n",
      "CPU times: user 49.9 s, sys: 2.71 s, total: 52.6 s\n",
      "Wall time: 14.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "build_layer_1(30,rotated_X_train,rotated_Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function for constructing the multi layer perceptron\n",
    "# 2 Hidden Layers\n",
    "def build_layer_2(nb_epoch,X_train,Y_train):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, input_shape=(784,)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(nb_classes))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "    model.fit(X_train, Y_train, batch_size=128, nb_epoch=nb_epoch,verbose=1,\n",
    "              validation_data=(X_test, Y_test))\n",
    "          \n",
    "\n",
    "    score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "    print('Test score:', score[0])\n",
    "    print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/prajwal967/anaconda3/lib/python3.6/site-packages/keras/models.py:874: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2000 samples, validate on 6000 samples\n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 1s 302us/step - loss: 0.7307 - acc: 0.8725 - val_loss: 0.1038 - val_acc: 0.9643\n",
      "Test score: 0.10378973338\n",
      "Test accuracy: 0.964333333333\n",
      "CPU times: user 3 s, sys: 171 ms, total: 3.17 s\n",
      "Wall time: 1.21 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "build_layer_2(1,X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/prajwal967/anaconda3/lib/python3.6/site-packages/keras/models.py:874: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6000 samples, validate on 6000 samples\n",
      "Epoch 1/1\n",
      "6000/6000 [==============================] - 1s 177us/step - loss: 0.2496 - acc: 0.9217 - val_loss: 0.2768 - val_acc: 0.9185\n",
      "Test score: 0.27684322523\n",
      "Test accuracy: 0.9185\n",
      "CPU times: user 4.55 s, sys: 305 ms, total: 4.85 s\n",
      "Wall time: 1.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "build_layer_2(1,rotated_X_train,rotated_Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/prajwal967/anaconda3/lib/python3.6/site-packages/keras/models.py:874: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2000 samples, validate on 6000 samples\n",
      "Epoch 1/5\n",
      "2000/2000 [==============================] - ETA: 0s - loss: 0.6008 - acc: 0.876 - 1s 339us/step - loss: 0.5488 - acc: 0.8860 - val_loss: 0.0722 - val_acc: 0.9772\n",
      "Epoch 2/5\n",
      "2000/2000 [==============================] - 0s 180us/step - loss: 0.1174 - acc: 0.9555 - val_loss: 0.0433 - val_acc: 0.9855\n",
      "Epoch 3/5\n",
      "2000/2000 [==============================] - 0s 187us/step - loss: 0.0595 - acc: 0.9740 - val_loss: 0.0461 - val_acc: 0.9825\n",
      "Epoch 4/5\n",
      "2000/2000 [==============================] - 0s 183us/step - loss: 0.0657 - acc: 0.9745 - val_loss: 0.0833 - val_acc: 0.9738\n",
      "Epoch 5/5\n",
      "2000/2000 [==============================] - 0s 180us/step - loss: 0.0187 - acc: 0.9915 - val_loss: 0.0406 - val_acc: 0.9877\n",
      "Test score: 0.0406126657825\n",
      "Test accuracy: 0.987666666667\n",
      "CPU times: user 8.31 s, sys: 487 ms, total: 8.8 s\n",
      "Wall time: 2.68 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "build_layer_2(5,X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/prajwal967/anaconda3/lib/python3.6/site-packages/keras/models.py:874: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6000 samples, validate on 6000 samples\n",
      "Epoch 1/5\n",
      "6000/6000 [==============================] - 1s 184us/step - loss: 0.2634 - acc: 0.9177 - val_loss: 0.0607 - val_acc: 0.9798\n",
      "Epoch 2/5\n",
      "6000/6000 [==============================] - 1s 128us/step - loss: 0.0796 - acc: 0.9703 - val_loss: 0.0669 - val_acc: 0.9785\n",
      "Epoch 3/5\n",
      "6000/6000 [==============================] - 1s 127us/step - loss: 0.0602 - acc: 0.9802 - val_loss: 0.0528 - val_acc: 0.9835\n",
      "Epoch 4/5\n",
      "6000/6000 [==============================] - 1s 132us/step - loss: 0.0379 - acc: 0.9867 - val_loss: 0.0589 - val_acc: 0.9865\n",
      "Epoch 5/5\n",
      "6000/6000 [==============================] - 1s 129us/step - loss: 0.0260 - acc: 0.9903 - val_loss: 0.0349 - val_acc: 0.9915\n",
      "Test score: 0.0348663751341\n",
      "Test accuracy: 0.9915\n",
      "CPU times: user 15.3 s, sys: 1.02 s, total: 16.4 s\n",
      "Wall time: 4.84 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "build_layer_2(5,rotated_X_train,rotated_Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/prajwal967/anaconda3/lib/python3.6/site-packages/keras/models.py:874: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2000 samples, validate on 6000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 1s 372us/step - loss: 0.9378 - acc: 0.8455 - val_loss: 0.0680 - val_acc: 0.9757\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 178us/step - loss: 0.0638 - acc: 0.9780 - val_loss: 0.2560 - val_acc: 0.9057\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 185us/step - loss: 0.0849 - acc: 0.9710 - val_loss: 0.0904 - val_acc: 0.9672\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 190us/step - loss: 0.0317 - acc: 0.9890 - val_loss: 0.2110 - val_acc: 0.9317\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 187us/step - loss: 0.0778 - acc: 0.9775 - val_loss: 0.0621 - val_acc: 0.9795\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 186us/step - loss: 0.0094 - acc: 0.9970 - val_loss: 0.0485 - val_acc: 0.9852\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 190us/step - loss: 0.0109 - acc: 0.9970 - val_loss: 0.0891 - val_acc: 0.9787\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 197us/step - loss: 0.0178 - acc: 0.9960 - val_loss: 0.0838 - val_acc: 0.9843\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 192us/step - loss: 0.0441 - acc: 0.9870 - val_loss: 0.0501 - val_acc: 0.9862\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 195us/step - loss: 0.0026 - acc: 0.9995 - val_loss: 0.0630 - val_acc: 0.9848\n",
      "Test score: 0.0629880966658\n",
      "Test accuracy: 0.984833333333\n",
      "CPU times: user 15 s, sys: 873 ms, total: 15.9 s\n",
      "Wall time: 4.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "build_layer_2(10,X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/prajwal967/anaconda3/lib/python3.6/site-packages/keras/models.py:874: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6000 samples, validate on 6000 samples\n",
      "Epoch 1/10\n",
      "6000/6000 [==============================] - 1s 198us/step - loss: 0.2437 - acc: 0.9183 - val_loss: 0.3389 - val_acc: 0.9112\n",
      "Epoch 2/10\n",
      "6000/6000 [==============================] - 1s 129us/step - loss: 0.0807 - acc: 0.9728 - val_loss: 0.0309 - val_acc: 0.9910\n",
      "Epoch 3/10\n",
      "6000/6000 [==============================] - 1s 135us/step - loss: 0.0528 - acc: 0.9822 - val_loss: 0.0677 - val_acc: 0.9833\n",
      "Epoch 4/10\n",
      "6000/6000 [==============================] - 1s 125us/step - loss: 0.0406 - acc: 0.9858 - val_loss: 0.0379 - val_acc: 0.9907\n",
      "Epoch 5/10\n",
      "6000/6000 [==============================] - 1s 135us/step - loss: 0.0271 - acc: 0.9895 - val_loss: 0.0908 - val_acc: 0.9857\n",
      "Epoch 6/10\n",
      "6000/6000 [==============================] - 1s 136us/step - loss: 0.0220 - acc: 0.9928 - val_loss: 0.0417 - val_acc: 0.9927\n",
      "Epoch 7/10\n",
      "6000/6000 [==============================] - 1s 131us/step - loss: 0.0223 - acc: 0.9935 - val_loss: 0.1058 - val_acc: 0.9827\n",
      "Epoch 8/10\n",
      "6000/6000 [==============================] - 1s 124us/step - loss: 0.0110 - acc: 0.9967 - val_loss: 0.0506 - val_acc: 0.9927\n",
      "Epoch 9/10\n",
      "6000/6000 [==============================] - 1s 127us/step - loss: 0.0092 - acc: 0.9963 - val_loss: 0.0893 - val_acc: 0.9882\n",
      "Epoch 10/10\n",
      "6000/6000 [==============================] - 1s 124us/step - loss: 0.0109 - acc: 0.9972 - val_loss: 0.0758 - val_acc: 0.9888\n",
      "Test score: 0.075845753372\n",
      "Test accuracy: 0.988833333333\n",
      "CPU times: user 29.2 s, sys: 1.96 s, total: 31.1 s\n",
      "Wall time: 8.72 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "build_layer_2(10,rotated_X_train,rotated_Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/prajwal967/anaconda3/lib/python3.6/site-packages/keras/models.py:874: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2000 samples, validate on 6000 samples\n",
      "Epoch 1/30\n",
      "2000/2000 [==============================] - 1s 403us/step - loss: 1.0326 - acc: 0.8500 - val_loss: 0.0900 - val_acc: 0.9663\n",
      "Epoch 2/30\n",
      "2000/2000 [==============================] - 0s 183us/step - loss: 0.0831 - acc: 0.9710 - val_loss: 0.0354 - val_acc: 0.9895\n",
      "Epoch 3/30\n",
      "2000/2000 [==============================] - 0s 191us/step - loss: 0.1642 - acc: 0.9485 - val_loss: 0.0693 - val_acc: 0.9762\n",
      "Epoch 4/30\n",
      "2000/2000 [==============================] - 0s 192us/step - loss: 0.0296 - acc: 0.9890 - val_loss: 0.0493 - val_acc: 0.9835\n",
      "Epoch 5/30\n",
      "2000/2000 [==============================] - 0s 184us/step - loss: 0.0214 - acc: 0.9925 - val_loss: 0.0432 - val_acc: 0.9882\n",
      "Epoch 6/30\n",
      "2000/2000 [==============================] - 0s 189us/step - loss: 0.0201 - acc: 0.9930 - val_loss: 0.0550 - val_acc: 0.9817\n",
      "Epoch 7/30\n",
      "2000/2000 [==============================] - 0s 193us/step - loss: 0.0280 - acc: 0.9920 - val_loss: 0.0513 - val_acc: 0.9857\n",
      "Epoch 8/30\n",
      "2000/2000 [==============================] - 1s 332us/step - loss: 0.0213 - acc: 0.9920 - val_loss: 0.3314 - val_acc: 0.9290\n",
      "Epoch 9/30\n",
      "2000/2000 [==============================] - 1s 354us/step - loss: 0.0158 - acc: 0.9955 - val_loss: 0.0544 - val_acc: 0.9860\n",
      "Epoch 10/30\n",
      "2000/2000 [==============================] - 1s 260us/step - loss: 0.0061 - acc: 0.9965 - val_loss: 0.2373 - val_acc: 0.9503\n",
      "Epoch 11/30\n",
      "2000/2000 [==============================] - 0s 199us/step - loss: 0.0189 - acc: 0.9935 - val_loss: 0.0683 - val_acc: 0.9860\n",
      "Epoch 12/30\n",
      "2000/2000 [==============================] - 0s 207us/step - loss: 0.0034 - acc: 0.9990 - val_loss: 0.0575 - val_acc: 0.9863\n",
      "Epoch 13/30\n",
      "2000/2000 [==============================] - 0s 203us/step - loss: 5.7974e-04 - acc: 1.0000 - val_loss: 0.0719 - val_acc: 0.9863\n",
      "Epoch 14/30\n",
      "2000/2000 [==============================] - 0s 182us/step - loss: 0.0760 - acc: 0.9795 - val_loss: 0.0989 - val_acc: 0.9837\n",
      "Epoch 15/30\n",
      "2000/2000 [==============================] - 0s 190us/step - loss: 8.7326e-04 - acc: 0.9995 - val_loss: 0.1106 - val_acc: 0.9803\n",
      "Epoch 16/30\n",
      "2000/2000 [==============================] - 0s 190us/step - loss: 1.5854e-04 - acc: 1.0000 - val_loss: 0.0986 - val_acc: 0.9823\n",
      "Epoch 17/30\n",
      "2000/2000 [==============================] - 0s 190us/step - loss: 1.3468e-04 - acc: 1.0000 - val_loss: 0.0909 - val_acc: 0.9847\n",
      "Epoch 18/30\n",
      "2000/2000 [==============================] - 0s 181us/step - loss: 6.2713e-05 - acc: 1.0000 - val_loss: 0.0914 - val_acc: 0.9845\n",
      "Epoch 19/30\n",
      "2000/2000 [==============================] - 0s 174us/step - loss: 4.8338e-05 - acc: 1.0000 - val_loss: 0.0845 - val_acc: 0.9867\n",
      "Epoch 20/30\n",
      "2000/2000 [==============================] - 0s 184us/step - loss: 0.0023 - acc: 0.9990 - val_loss: 0.1833 - val_acc: 0.9755\n",
      "Epoch 21/30\n",
      "2000/2000 [==============================] - 0s 186us/step - loss: 0.0102 - acc: 0.9965 - val_loss: 0.0969 - val_acc: 0.9837\n",
      "Epoch 22/30\n",
      "2000/2000 [==============================] - 0s 192us/step - loss: 8.8272e-04 - acc: 0.9995 - val_loss: 0.0941 - val_acc: 0.9845\n",
      "Epoch 23/30\n",
      "2000/2000 [==============================] - 0s 182us/step - loss: 0.0023 - acc: 0.9990 - val_loss: 0.1036 - val_acc: 0.9835\n",
      "Epoch 24/30\n",
      "2000/2000 [==============================] - 0s 183us/step - loss: 9.9406e-04 - acc: 0.9995 - val_loss: 0.1038 - val_acc: 0.9845\n",
      "Epoch 25/30\n",
      "2000/2000 [==============================] - 0s 183us/step - loss: 5.2288e-05 - acc: 1.0000 - val_loss: 0.1021 - val_acc: 0.9853\n",
      "Epoch 26/30\n",
      "2000/2000 [==============================] - 0s 206us/step - loss: 0.0147 - acc: 0.9955 - val_loss: 0.1271 - val_acc: 0.9822\n",
      "Epoch 27/30\n",
      "2000/2000 [==============================] - 0s 198us/step - loss: 1.0447e-04 - acc: 1.0000 - val_loss: 0.1317 - val_acc: 0.9820\n",
      "Epoch 28/30\n",
      "2000/2000 [==============================] - 0s 186us/step - loss: 0.0281 - acc: 0.9950 - val_loss: 0.0963 - val_acc: 0.9877\n",
      "Epoch 29/30\n",
      "2000/2000 [==============================] - 0s 197us/step - loss: 6.3049e-05 - acc: 1.0000 - val_loss: 0.1223 - val_acc: 0.9842\n",
      "Epoch 30/30\n",
      "2000/2000 [==============================] - 0s 184us/step - loss: 2.6678e-05 - acc: 1.0000 - val_loss: 0.0980 - val_acc: 0.9887\n",
      "Test score: 0.0980378216563\n",
      "Test accuracy: 0.988666666667\n",
      "CPU times: user 45.5 s, sys: 2.92 s, total: 48.4 s\n",
      "Wall time: 13.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "build_layer_2(30,X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/prajwal967/anaconda3/lib/python3.6/site-packages/keras/models.py:874: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6000 samples, validate on 6000 samples\n",
      "Epoch 1/30\n",
      "6000/6000 [==============================] - 1s 202us/step - loss: 0.2324 - acc: 0.9292 - val_loss: 0.0415 - val_acc: 0.9890\n",
      "Epoch 2/30\n",
      "6000/6000 [==============================] - 1s 125us/step - loss: 0.0825 - acc: 0.9743 - val_loss: 0.0584 - val_acc: 0.9813\n",
      "Epoch 3/30\n",
      "6000/6000 [==============================] - 1s 129us/step - loss: 0.0494 - acc: 0.9827 - val_loss: 0.1397 - val_acc: 0.9652\n",
      "Epoch 4/30\n",
      "6000/6000 [==============================] - 1s 121us/step - loss: 0.0312 - acc: 0.9895 - val_loss: 0.0520 - val_acc: 0.9897\n",
      "Epoch 5/30\n",
      "6000/6000 [==============================] - 1s 133us/step - loss: 0.0302 - acc: 0.9890 - val_loss: 0.0812 - val_acc: 0.9833\n",
      "Epoch 6/30\n",
      "6000/6000 [==============================] - 1s 128us/step - loss: 0.0259 - acc: 0.9917 - val_loss: 0.0363 - val_acc: 0.9928\n",
      "Epoch 7/30\n",
      "6000/6000 [==============================] - 1s 144us/step - loss: 0.0187 - acc: 0.9950 - val_loss: 0.0727 - val_acc: 0.9883\n",
      "Epoch 8/30\n",
      "6000/6000 [==============================] - 1s 124us/step - loss: 0.0232 - acc: 0.9938 - val_loss: 0.0952 - val_acc: 0.9853\n",
      "Epoch 9/30\n",
      "6000/6000 [==============================] - 1s 122us/step - loss: 0.0076 - acc: 0.9973 - val_loss: 0.0983 - val_acc: 0.9863\n",
      "Epoch 10/30\n",
      "6000/6000 [==============================] - 1s 128us/step - loss: 0.0195 - acc: 0.9958 - val_loss: 0.0702 - val_acc: 0.9900\n",
      "Epoch 11/30\n",
      "6000/6000 [==============================] - 1s 131us/step - loss: 0.0120 - acc: 0.9975 - val_loss: 0.0821 - val_acc: 0.9887\n",
      "Epoch 12/30\n",
      "6000/6000 [==============================] - 1s 125us/step - loss: 0.0060 - acc: 0.9977 - val_loss: 0.1008 - val_acc: 0.9883\n",
      "Epoch 13/30\n",
      "6000/6000 [==============================] - 1s 122us/step - loss: 0.0063 - acc: 0.9983 - val_loss: 0.1396 - val_acc: 0.9860\n",
      "Epoch 14/30\n",
      "6000/6000 [==============================] - 1s 129us/step - loss: 0.0040 - acc: 0.9993 - val_loss: 0.1208 - val_acc: 0.9863\n",
      "Epoch 15/30\n",
      "6000/6000 [==============================] - 1s 128us/step - loss: 0.0080 - acc: 0.9987 - val_loss: 0.4894 - val_acc: 0.9563\n",
      "Epoch 16/30\n",
      "6000/6000 [==============================] - 1s 126us/step - loss: 0.0177 - acc: 0.9973 - val_loss: 0.1540 - val_acc: 0.9855\n",
      "Epoch 17/30\n",
      "6000/6000 [==============================] - 1s 124us/step - loss: 0.0091 - acc: 0.9985 - val_loss: 0.1924 - val_acc: 0.9820\n",
      "Epoch 18/30\n",
      "6000/6000 [==============================] - 1s 124us/step - loss: 0.0051 - acc: 0.9988 - val_loss: 0.1317 - val_acc: 0.9873\n",
      "Epoch 19/30\n",
      "6000/6000 [==============================] - 1s 132us/step - loss: 0.0062 - acc: 0.9978 - val_loss: 0.1621 - val_acc: 0.9850\n",
      "Epoch 20/30\n",
      "6000/6000 [==============================] - 1s 121us/step - loss: 0.0015 - acc: 0.9993 - val_loss: 0.1385 - val_acc: 0.9868\n",
      "Epoch 21/30\n",
      "6000/6000 [==============================] - 1s 121us/step - loss: 0.0121 - acc: 0.9978 - val_loss: 0.0870 - val_acc: 0.9922\n",
      "Epoch 22/30\n",
      "6000/6000 [==============================] - 1s 122us/step - loss: 0.0022 - acc: 0.9993 - val_loss: 0.1422 - val_acc: 0.9888\n",
      "Epoch 23/30\n",
      "6000/6000 [==============================] - 1s 120us/step - loss: 0.0066 - acc: 0.9992 - val_loss: 0.2776 - val_acc: 0.9762\n",
      "Epoch 24/30\n",
      "6000/6000 [==============================] - 1s 129us/step - loss: 0.0160 - acc: 0.9977 - val_loss: 0.1717 - val_acc: 0.9863\n",
      "Epoch 25/30\n",
      "6000/6000 [==============================] - 1s 119us/step - loss: 0.0091 - acc: 0.9982 - val_loss: 0.1243 - val_acc: 0.9893\n",
      "Epoch 26/30\n",
      "6000/6000 [==============================] - 1s 121us/step - loss: 0.0030 - acc: 0.9998 - val_loss: 0.1165 - val_acc: 0.9895\n",
      "Epoch 27/30\n",
      "6000/6000 [==============================] - 1s 128us/step - loss: 0.0163 - acc: 0.9975 - val_loss: 0.3553 - val_acc: 0.9705\n",
      "Epoch 28/30\n",
      "6000/6000 [==============================] - 1s 127us/step - loss: 0.0039 - acc: 0.9992 - val_loss: 0.1173 - val_acc: 0.9902\n",
      "Epoch 29/30\n",
      "6000/6000 [==============================] - 1s 123us/step - loss: 0.0083 - acc: 0.9987 - val_loss: 0.0878 - val_acc: 0.9922\n",
      "Epoch 30/30\n",
      "6000/6000 [==============================] - 1s 122us/step - loss: 0.0049 - acc: 0.9993 - val_loss: 0.0828 - val_acc: 0.9928\n",
      "Test score: 0.0827601716926\n",
      "Test accuracy: 0.992833333333\n",
      "CPU times: user 1min 21s, sys: 6.07 s, total: 1min 27s\n",
      "Wall time: 23.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "build_layer_2(30,rotated_X_train,rotated_Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function for constructing the multi layer perceptron\n",
    "## Hidden Layers\n",
    "def build_layer_3(nb_epoch,X_train,Y_train):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, input_shape=(784,)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(nb_classes))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "    model.fit(X_train, Y_train, batch_size=128, nb_epoch=nb_epoch,verbose=1,\n",
    "              validation_data=(X_test, Y_test))\n",
    "          \n",
    "\n",
    "    score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "    print('Test score:', score[0])\n",
    "    print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/prajwal967/anaconda3/lib/python3.6/site-packages/keras/models.py:874: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2000 samples, validate on 6000 samples\n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 1s 491us/step - loss: 0.6790 - acc: 0.8290 - val_loss: 0.1012 - val_acc: 0.9638\n",
      "Test score: 0.101154099841\n",
      "Test accuracy: 0.963833333333\n",
      "CPU times: user 4.08 s, sys: 262 ms, total: 4.34 s\n",
      "Wall time: 1.62 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "build_layer_3(1,X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/prajwal967/anaconda3/lib/python3.6/site-packages/keras/models.py:874: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6000 samples, validate on 6000 samples\n",
      "Epoch 1/1\n",
      "6000/6000 [==============================] - 2s 257us/step - loss: 0.3337 - acc: 0.9060 - val_loss: 0.0887 - val_acc: 0.9723\n",
      "Test score: 0.0886738774649\n",
      "Test accuracy: 0.972333333333\n",
      "CPU times: user 6.22 s, sys: 484 ms, total: 6.7 s\n",
      "Wall time: 2.29 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "build_layer_3(1,rotated_X_train,rotated_Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/prajwal967/anaconda3/lib/python3.6/site-packages/keras/models.py:874: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2000 samples, validate on 6000 samples\n",
      "Epoch 1/5\n",
      "2000/2000 [==============================] - 1s 571us/step - loss: 0.9250 - acc: 0.8330 - val_loss: 0.0717 - val_acc: 0.9732\n",
      "Epoch 2/5\n",
      "2000/2000 [==============================] - 1s 263us/step - loss: 0.1180 - acc: 0.9575 - val_loss: 0.1708 - val_acc: 0.9367\n",
      "Epoch 3/5\n",
      "2000/2000 [==============================] - 1s 257us/step - loss: 0.0561 - acc: 0.9790 - val_loss: 0.0397 - val_acc: 0.9847\n",
      "Epoch 4/5\n",
      "2000/2000 [==============================] - 1s 265us/step - loss: 0.1411 - acc: 0.9540 - val_loss: 0.0505 - val_acc: 0.9807\n",
      "Epoch 5/5\n",
      "2000/2000 [==============================] - 0s 245us/step - loss: 0.0223 - acc: 0.9930 - val_loss: 0.0456 - val_acc: 0.9837\n",
      "Test score: 0.0455576283092\n",
      "Test accuracy: 0.983666666667\n",
      "CPU times: user 11.2 s, sys: 816 ms, total: 12 s\n",
      "Wall time: 4.03 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "build_layer_3(5,X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/prajwal967/anaconda3/lib/python3.6/site-packages/keras/models.py:874: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6000 samples, validate on 6000 samples\n",
      "Epoch 1/5\n",
      "6000/6000 [==============================] - 2s 283us/step - loss: 0.2896 - acc: 0.9103 - val_loss: 0.0549 - val_acc: 0.9838\n",
      "Epoch 2/5\n",
      "6000/6000 [==============================] - 1s 173us/step - loss: 0.1110 - acc: 0.9638 - val_loss: 0.0278 - val_acc: 0.9890\n",
      "Epoch 3/5\n",
      "6000/6000 [==============================] - 1s 168us/step - loss: 0.0518 - acc: 0.9823 - val_loss: 0.0613 - val_acc: 0.9842\n",
      "Epoch 4/5\n",
      "6000/6000 [==============================] - 1s 182us/step - loss: 0.0542 - acc: 0.9845 - val_loss: 0.0243 - val_acc: 0.9937\n",
      "Epoch 5/5\n",
      "6000/6000 [==============================] - 1s 185us/step - loss: 0.0323 - acc: 0.9900 - val_loss: 0.1597 - val_acc: 0.9710\n",
      "Test score: 0.159722540914\n",
      "Test accuracy: 0.971\n",
      "CPU times: user 20.7 s, sys: 1.71 s, total: 22.4 s\n",
      "Wall time: 6.64 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "build_layer_3(5,rotated_X_train,rotated_Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/prajwal967/anaconda3/lib/python3.6/site-packages/keras/models.py:874: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2000 samples, validate on 6000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 1s 571us/step - loss: 0.7576 - acc: 0.8255 - val_loss: 0.0454 - val_acc: 0.9832\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 245us/step - loss: 0.0994 - acc: 0.9720 - val_loss: 0.0367 - val_acc: 0.9852\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 241us/step - loss: 0.0817 - acc: 0.9705 - val_loss: 0.0413 - val_acc: 0.9847\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 1s 261us/step - loss: 0.0398 - acc: 0.9860 - val_loss: 0.0360 - val_acc: 0.9885\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 244us/step - loss: 0.0330 - acc: 0.9870 - val_loss: 0.0586 - val_acc: 0.9837\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 242us/step - loss: 0.0472 - acc: 0.9830 - val_loss: 0.0388 - val_acc: 0.9905\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 1s 258us/step - loss: 0.0094 - acc: 0.9970 - val_loss: 0.0648 - val_acc: 0.9843\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 1s 256us/step - loss: 0.0321 - acc: 0.9890 - val_loss: 0.0623 - val_acc: 0.9820\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 241us/step - loss: 0.0071 - acc: 0.9975 - val_loss: 0.0413 - val_acc: 0.9908\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 1s 252us/step - loss: 0.0338 - acc: 0.9930 - val_loss: 0.7223 - val_acc: 0.8863\n",
      "Test score: 0.722320004796\n",
      "Test accuracy: 0.886333333333\n",
      "CPU times: user 20 s, sys: 1.42 s, total: 21.4 s\n",
      "Wall time: 6.33 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "build_layer_3(10,X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/prajwal967/anaconda3/lib/python3.6/site-packages/keras/models.py:874: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6000 samples, validate on 6000 samples\n",
      "Epoch 1/10\n",
      "6000/6000 [==============================] - 2s 289us/step - loss: 0.3167 - acc: 0.9098 - val_loss: 0.1596 - val_acc: 0.9523\n",
      "Epoch 2/10\n",
      "6000/6000 [==============================] - 1s 167us/step - loss: 0.0877 - acc: 0.9678 - val_loss: 0.1313 - val_acc: 0.9630\n",
      "Epoch 3/10\n",
      "6000/6000 [==============================] - 1s 193us/step - loss: 0.0635 - acc: 0.9795 - val_loss: 0.0873 - val_acc: 0.9798\n",
      "Epoch 4/10\n",
      "6000/6000 [==============================] - 1s 169us/step - loss: 0.0454 - acc: 0.9852 - val_loss: 0.0359 - val_acc: 0.9905\n",
      "Epoch 5/10\n",
      "6000/6000 [==============================] - 1s 179us/step - loss: 0.0394 - acc: 0.9867 - val_loss: 0.0427 - val_acc: 0.9905\n",
      "Epoch 6/10\n",
      "6000/6000 [==============================] - 1s 192us/step - loss: 0.0338 - acc: 0.9887 - val_loss: 0.0486 - val_acc: 0.9865\n",
      "Epoch 7/10\n",
      "6000/6000 [==============================] - 2s 256us/step - loss: 0.0212 - acc: 0.9950 - val_loss: 0.1606 - val_acc: 0.9790\n",
      "Epoch 8/10\n",
      "6000/6000 [==============================] - 2s 272us/step - loss: 0.0257 - acc: 0.9937 - val_loss: 0.0472 - val_acc: 0.9907\n",
      "Epoch 9/10\n",
      "6000/6000 [==============================] - 2s 254us/step - loss: 0.0125 - acc: 0.9962 - val_loss: 0.0955 - val_acc: 0.9857\n",
      "Epoch 10/10\n",
      "6000/6000 [==============================] - 1s 194us/step - loss: 0.0119 - acc: 0.9958 - val_loss: 0.0682 - val_acc: 0.9898\n",
      "Test score: 0.0681727476969\n",
      "Test accuracy: 0.989833333333\n",
      "CPU times: user 47.1 s, sys: 4.12 s, total: 51.3 s\n",
      "Wall time: 14 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "build_layer_3(10,rotated_X_train,rotated_Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/prajwal967/anaconda3/lib/python3.6/site-packages/keras/models.py:874: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2000 samples, validate on 6000 samples\n",
      "Epoch 1/30\n",
      "2000/2000 [==============================] - 2s 919us/step - loss: 0.8255 - acc: 0.8285 - val_loss: 0.0571 - val_acc: 0.9778\n",
      "Epoch 2/30\n",
      "2000/2000 [==============================] - 1s 329us/step - loss: 0.0754 - acc: 0.9730 - val_loss: 0.0624 - val_acc: 0.9765\n",
      "Epoch 3/30\n",
      "2000/2000 [==============================] - 0s 245us/step - loss: 0.0824 - acc: 0.9675 - val_loss: 0.1140 - val_acc: 0.9627\n",
      "Epoch 4/30\n",
      "2000/2000 [==============================] - 1s 396us/step - loss: 0.0349 - acc: 0.9875 - val_loss: 0.0584 - val_acc: 0.9793\n",
      "Epoch 5/30\n",
      "2000/2000 [==============================] - 1s 440us/step - loss: 0.1436 - acc: 0.9680 - val_loss: 0.0468 - val_acc: 0.9842\n",
      "Epoch 6/30\n",
      "2000/2000 [==============================] - 1s 330us/step - loss: 0.0139 - acc: 0.9950 - val_loss: 0.0412 - val_acc: 0.9865\n",
      "Epoch 7/30\n",
      "2000/2000 [==============================] - 1s 251us/step - loss: 0.0076 - acc: 0.9975 - val_loss: 0.0631 - val_acc: 0.9827\n",
      "Epoch 8/30\n",
      "2000/2000 [==============================] - 1s 329us/step - loss: 0.0501 - acc: 0.9845 - val_loss: 0.0506 - val_acc: 0.9863\n",
      "Epoch 9/30\n",
      "2000/2000 [==============================] - 1s 356us/step - loss: 0.0036 - acc: 0.9990 - val_loss: 0.0621 - val_acc: 0.9858\n",
      "Epoch 10/30\n",
      "2000/2000 [==============================] - 1s 350us/step - loss: 0.0381 - acc: 0.9895 - val_loss: 0.1483 - val_acc: 0.9673\n",
      "Epoch 11/30\n",
      "2000/2000 [==============================] - 1s 444us/step - loss: 0.0049 - acc: 0.9980 - val_loss: 0.0873 - val_acc: 0.9798\n",
      "Epoch 12/30\n",
      "2000/2000 [==============================] - 1s 303us/step - loss: 0.0051 - acc: 0.9985 - val_loss: 0.0742 - val_acc: 0.9852\n",
      "Epoch 13/30\n",
      "2000/2000 [==============================] - 1s 383us/step - loss: 5.3350e-04 - acc: 1.0000 - val_loss: 0.0938 - val_acc: 0.9855\n",
      "Epoch 14/30\n",
      "2000/2000 [==============================] - 1s 304us/step - loss: 0.0167 - acc: 0.9955 - val_loss: 0.0737 - val_acc: 0.9857\n",
      "Epoch 15/30\n",
      "2000/2000 [==============================] - 1s 299us/step - loss: 0.0284 - acc: 0.9945 - val_loss: 0.1282 - val_acc: 0.9858\n",
      "Epoch 16/30\n",
      "2000/2000 [==============================] - 1s 389us/step - loss: 0.0243 - acc: 0.9950 - val_loss: 0.0922 - val_acc: 0.9808\n",
      "Epoch 17/30\n",
      "2000/2000 [==============================] - 1s 279us/step - loss: 2.1453e-04 - acc: 1.0000 - val_loss: 0.1131 - val_acc: 0.9783\n",
      "Epoch 18/30\n",
      "2000/2000 [==============================] - 1s 260us/step - loss: 3.3621e-04 - acc: 1.0000 - val_loss: 0.0993 - val_acc: 0.9828\n",
      "Epoch 19/30\n",
      "2000/2000 [==============================] - 1s 336us/step - loss: 5.6754e-05 - acc: 1.0000 - val_loss: 0.1045 - val_acc: 0.9848\n",
      "Epoch 20/30\n",
      "2000/2000 [==============================] - 1s 329us/step - loss: 1.2605e-05 - acc: 1.0000 - val_loss: 0.1229 - val_acc: 0.9840\n",
      "Epoch 21/30\n",
      "2000/2000 [==============================] - 1s 319us/step - loss: 0.0688 - acc: 0.9890 - val_loss: 1.5566 - val_acc: 0.8443\n",
      "Epoch 22/30\n",
      "2000/2000 [==============================] - 1s 264us/step - loss: 0.0886 - acc: 0.9905 - val_loss: 0.1173 - val_acc: 0.9830\n",
      "Epoch 23/30\n",
      "2000/2000 [==============================] - 1s 387us/step - loss: 0.0019 - acc: 0.9990 - val_loss: 0.1218 - val_acc: 0.9823\n",
      "Epoch 24/30\n",
      "2000/2000 [==============================] - 1s 353us/step - loss: 0.0052 - acc: 0.9980 - val_loss: 0.0892 - val_acc: 0.9868\n",
      "Epoch 25/30\n",
      "2000/2000 [==============================] - 1s 312us/step - loss: 2.1754e-05 - acc: 1.0000 - val_loss: 0.1109 - val_acc: 0.9852\n",
      "Epoch 26/30\n",
      "2000/2000 [==============================] - 1s 272us/step - loss: 2.4826e-05 - acc: 1.0000 - val_loss: 0.0915 - val_acc: 0.9885\n",
      "Epoch 27/30\n",
      "2000/2000 [==============================] - 1s 267us/step - loss: 6.4137e-06 - acc: 1.0000 - val_loss: 0.1162 - val_acc: 0.9860\n",
      "Epoch 28/30\n",
      "2000/2000 [==============================] - 1s 289us/step - loss: 2.2707e-06 - acc: 1.0000 - val_loss: 0.1298 - val_acc: 0.9852\n",
      "Epoch 29/30\n",
      "2000/2000 [==============================] - 1s 296us/step - loss: 6.4167e-07 - acc: 1.0000 - val_loss: 0.1353 - val_acc: 0.9847\n",
      "Epoch 30/30\n",
      "2000/2000 [==============================] - 1s 287us/step - loss: 2.3534e-06 - acc: 1.0000 - val_loss: 0.1813 - val_acc: 0.9823\n",
      "Test score: 0.181339349609\n",
      "Test accuracy: 0.982333333333\n",
      "CPU times: user 1min 10s, sys: 5.75 s, total: 1min 16s\n",
      "Wall time: 21.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "build_layer_3(30,X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/prajwal967/anaconda3/lib/python3.6/site-packages/keras/models.py:874: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6000 samples, validate on 6000 samples\n",
      "Epoch 1/30\n",
      "6000/6000 [==============================] - 2s 322us/step - loss: 0.3315 - acc: 0.9082 - val_loss: 0.2059 - val_acc: 0.9373\n",
      "Epoch 2/30\n",
      "6000/6000 [==============================] - 1s 192us/step - loss: 0.0836 - acc: 0.9690 - val_loss: 0.0387 - val_acc: 0.9892\n",
      "Epoch 3/30\n",
      "6000/6000 [==============================] - 1s 187us/step - loss: 0.0642 - acc: 0.9790 - val_loss: 0.1414 - val_acc: 0.9652\n",
      "Epoch 4/30\n",
      "6000/6000 [==============================] - 1s 185us/step - loss: 0.0393 - acc: 0.9873 - val_loss: 0.1619 - val_acc: 0.9685\n",
      "Epoch 5/30\n",
      "6000/6000 [==============================] - 1s 186us/step - loss: 0.0358 - acc: 0.9873 - val_loss: 0.0450 - val_acc: 0.9878\n",
      "Epoch 6/30\n",
      "6000/6000 [==============================] - 1s 189us/step - loss: 0.0237 - acc: 0.9920 - val_loss: 0.0885 - val_acc: 0.9823\n",
      "Epoch 7/30\n",
      "6000/6000 [==============================] - 1s 195us/step - loss: 0.0294 - acc: 0.9918 - val_loss: 0.1158 - val_acc: 0.9757\n",
      "Epoch 8/30\n",
      "6000/6000 [==============================] - 1s 187us/step - loss: 0.0187 - acc: 0.9947 - val_loss: 0.0537 - val_acc: 0.9897\n",
      "Epoch 9/30\n",
      "6000/6000 [==============================] - 1s 181us/step - loss: 0.0149 - acc: 0.9947 - val_loss: 0.1550 - val_acc: 0.9818\n",
      "Epoch 10/30\n",
      "6000/6000 [==============================] - 1s 178us/step - loss: 0.0194 - acc: 0.9933 - val_loss: 0.0752 - val_acc: 0.9887\n",
      "Epoch 11/30\n",
      "6000/6000 [==============================] - 1s 175us/step - loss: 0.0159 - acc: 0.9965 - val_loss: 0.0755 - val_acc: 0.9887\n",
      "Epoch 12/30\n",
      "6000/6000 [==============================] - 1s 196us/step - loss: 0.0099 - acc: 0.9975 - val_loss: 0.1149 - val_acc: 0.9858\n",
      "Epoch 13/30\n",
      "6000/6000 [==============================] - 1s 179us/step - loss: 0.0235 - acc: 0.9952 - val_loss: 0.0507 - val_acc: 0.9923\n",
      "Epoch 14/30\n",
      "6000/6000 [==============================] - 1s 183us/step - loss: 0.0110 - acc: 0.9965 - val_loss: 0.0686 - val_acc: 0.9890\n",
      "Epoch 15/30\n",
      "6000/6000 [==============================] - 1s 186us/step - loss: 0.0146 - acc: 0.9968 - val_loss: 0.2176 - val_acc: 0.9732\n",
      "Epoch 16/30\n",
      "6000/6000 [==============================] - 1s 185us/step - loss: 0.0061 - acc: 0.9988 - val_loss: 0.1855 - val_acc: 0.9835\n",
      "Epoch 17/30\n",
      "6000/6000 [==============================] - 1s 183us/step - loss: 0.0122 - acc: 0.9977 - val_loss: 0.1024 - val_acc: 0.9902\n",
      "Epoch 18/30\n",
      "6000/6000 [==============================] - 1s 186us/step - loss: 0.0047 - acc: 0.9987 - val_loss: 0.3151 - val_acc: 0.9722\n",
      "Epoch 19/30\n",
      "6000/6000 [==============================] - 1s 192us/step - loss: 0.0124 - acc: 0.9975 - val_loss: 0.1357 - val_acc: 0.9872\n",
      "Epoch 20/30\n",
      "6000/6000 [==============================] - 1s 193us/step - loss: 0.0083 - acc: 0.9982 - val_loss: 0.2543 - val_acc: 0.9792\n",
      "Epoch 21/30\n",
      "6000/6000 [==============================] - 1s 197us/step - loss: 0.0095 - acc: 0.9983 - val_loss: 0.1986 - val_acc: 0.9835\n",
      "Epoch 22/30\n",
      "6000/6000 [==============================] - 1s 215us/step - loss: 0.0142 - acc: 0.9980 - val_loss: 0.1161 - val_acc: 0.9898\n",
      "Epoch 23/30\n",
      "6000/6000 [==============================] - 1s 196us/step - loss: 0.0139 - acc: 0.9977 - val_loss: 0.1559 - val_acc: 0.9855\n",
      "Epoch 24/30\n",
      "6000/6000 [==============================] - 1s 186us/step - loss: 0.0051 - acc: 0.9993 - val_loss: 0.1617 - val_acc: 0.9855\n",
      "Epoch 25/30\n",
      "6000/6000 [==============================] - 1s 248us/step - loss: 0.0108 - acc: 0.9980 - val_loss: 0.0894 - val_acc: 0.9915\n",
      "Epoch 26/30\n",
      "6000/6000 [==============================] - 1s 206us/step - loss: 0.0060 - acc: 0.9990 - val_loss: 0.1215 - val_acc: 0.9885\n",
      "Epoch 27/30\n",
      "6000/6000 [==============================] - 1s 197us/step - loss: 0.0050 - acc: 0.9985 - val_loss: 0.1090 - val_acc: 0.9902\n",
      "Epoch 28/30\n",
      "6000/6000 [==============================] - 1s 209us/step - loss: 0.0108 - acc: 0.9983 - val_loss: 0.1564 - val_acc: 0.9863\n",
      "Epoch 29/30\n",
      "6000/6000 [==============================] - 1s 204us/step - loss: 0.0044 - acc: 0.9992 - val_loss: 0.1560 - val_acc: 0.9888\n",
      "Epoch 30/30\n",
      "6000/6000 [==============================] - 1s 202us/step - loss: 0.0035 - acc: 0.9993 - val_loss: 0.1521 - val_acc: 0.9877\n",
      "Test score: 0.152098201477\n",
      "Test accuracy: 0.987666666667\n",
      "CPU times: user 2min 5s, sys: 10.7 s, total: 2min 16s\n",
      "Wall time: 36.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "build_layer_3(30,rotated_X_train,rotated_Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
